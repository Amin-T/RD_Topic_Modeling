{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import liberaries and functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# plt.style.use('seaborn')\n",
    "# plt.rc('figure', autolayout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data sample \n",
    "RF_df = pd.read_csv(\"Data\\clean_docs_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1245475, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'ETM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ETM\\doc_topic_dist_3.pkl', 'rb') as f:\n",
    "    document_topic_dist = pickle.load(f).sort(descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and match the training indexes and the sample\n",
    "with open(\"ETM\\ETM_idx_train_100123.pkl\", 'rb') as f:\n",
    "    idx_train = pickle.load(f)\n",
    "\n",
    "RF_df.reset_index(inplace=True)\n",
    "RF_df[\"doc_idx\"] = pd.Series(data=range(len(idx_train)), index = idx_train)\n",
    "\n",
    "topics_df = RF_df.set_index('doc_idx').sort_index()\n",
    "\n",
    "topics_df[\"filing_dt\"] = pd.to_datetime(topics_df[\"filing_dt\"])\n",
    "topics_df[\"report_dt\"] = pd.to_datetime(topics_df[\"report_dt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df['rprt_length'] = topics_df['cleaned_txt'].map(lambda x: len(x.split()))\n",
    "\n",
    "# Get topics per RF\n",
    "topics_df[\"topic\"] = document_topic_dist.indices[:, 0]\n",
    "topics_df[\"Score\"] = document_topic_dist.values[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35468"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_df.groupby(topics_df.report_dt.dt.year)[\"CIK\"].nunique().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'T2V_H'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Top2Vec topics df\n",
    "t2v_df = (\n",
    "    pd.read_csv(\"Top2Vec\\T2V_df_H95.csv\")\n",
    "    .set_index(\"index\").drop(columns=['Docs'])\n",
    ")\n",
    "\n",
    "t2v_df.rename(columns={'Report_dt': 'report_dt', 'Report_dt': 'filing_dt'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df = pd.concat([RF_df, t2v_df[['Topic', 'Score', 'Topic_H', 'Score_H']]], axis=1)\n",
    "\n",
    "topics_df[\"filing_dt\"] = pd.to_datetime(topics_df[\"filing_dt\"])\n",
    "topics_df[\"report_dt\"] = pd.to_datetime(topics_df[\"report_dt\"])\n",
    "\n",
    "topics_df['rprt_length'] = topics_df['cleaned_txt'].map(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing observations with duplicated filing_dt and ryear\n",
    "sample = (\n",
    "    topics_df[[\"CIK\", \"filing_dt\", \"report_dt\"]]\n",
    "    .drop_duplicates()\n",
    "    .sort_values([\"CIK\", \"filing_dt\", \"report_dt\"])\n",
    ")\n",
    "\n",
    "sample.drop_duplicates([\"CIK\", \"filing_dt\"], keep='last', inplace=True)\n",
    "\n",
    "sample['ryear'] = sample['report_dt'].dt.year\n",
    "sample.drop_duplicates([\"CIK\", \"ryear\"], keep='first', inplace=True)\n",
    "\n",
    "sample.drop(columns='ryear', inplace=True)\n",
    "\n",
    "topics_df = (\n",
    "    topics_df.set_index([\"CIK\", \"filing_dt\", \"report_dt\"])\n",
    "    .loc[sample.apply(lambda x: (x[0], x[1], x[2]), axis=1).values]\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use historical SIC data for industry analysis\n",
    "sich = pd.read_csv(\n",
    "    filepath_or_buffer=\"Data\\Financials.csv\",\n",
    "    decimal=\".\", \n",
    "    thousands=',',\n",
    "    usecols=[\n",
    "        'cik', 'datadate', 'sich', 'sic'\n",
    "    ]\n",
    ").drop_duplicates()\n",
    "\n",
    "sich.sort_values(['cik', 'datadate'], inplace=True)\n",
    "sich[\"datadate\"] = pd.to_datetime(sich[\"datadate\"], format=\"%d/%m/%Y\")\n",
    "\n",
    "# Replace missing historical data with current data \n",
    "sich['sich'] = sich['sich'].fillna(sich['sic']).astype(int)\n",
    "\n",
    "topics_df = pd.merge(\n",
    "    left=topics_df,\n",
    "    right=sich[[\n",
    "        'cik', 'datadate', 'naicsh', 'sich']],\n",
    "    left_on=[\"CIK\", \"report_dt\"],\n",
    "    right_on=[\"cik\", \"datadate\"],\n",
    "    how=\"left\"\n",
    ").drop(columns=['datadate', 'cik'])\n",
    "\n",
    "topics_df['sich'] = topics_df['sich'].fillna(topics_df['SIC']).astype(int)\n",
    "\n",
    "# First 2 digits of SIC -> Major sector group\n",
    "topics_df['SIC'] = topics_df['sich'].map(lambda x: str(x)[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the records at the CIK-year level\n",
    "agg_tops = (\n",
    "    topics_df.groupby([\"CIK\", \"filing_dt\"])[['report_dt', 'Topic', 'Topic_H', 'rprt_length', 'SIC', 'Industry', 'category']]\n",
    "    .agg({\n",
    "        'report_dt': 'max', \n",
    "        'Topic': lambda l: set(l), \n",
    "        'Topic_H': lambda l: set(l), \n",
    "        'rprt_length': 'sum',\n",
    "        'SIC' : 'unique', \n",
    "        'Industry' : 'unique',\n",
    "        'category' : 'unique'\n",
    "    }).reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_tops[\"SIC\"] = agg_tops[\"SIC\"].map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables for H1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 'H1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For micro-topics\n",
    "# Shift records to compare every year with previous year\n",
    "agg_tops[\"shifted\"] = agg_tops.groupby(\"CIK\")['Topic'].shift(1)\n",
    "\n",
    "# For macro-topics\n",
    "# Shift records to compare every year with previous year\n",
    "agg_tops[\"shifted_H\"] = agg_tops.groupby(\"CIK\")['Topic_H'].shift(1)\n",
    "\n",
    "agg_tops.dropna(inplace=True)\n",
    "\n",
    "# Generate repeated, added and removed RFs\n",
    "agg_tops[\"repeated\"] = agg_tops.apply(lambda r: r['Topic'].intersection(r['shifted']), axis=1)\n",
    "agg_tops[\"added\"] = agg_tops.apply(lambda r: r['Topic'].difference(r['shifted']), axis=1)\n",
    "agg_tops[\"removed\"] = agg_tops.apply(lambda r: r['shifted'].difference(r['Topic']), axis=1)\n",
    "\n",
    "# Generate repeated, added and removed RFs\n",
    "agg_tops[\"repeated_H\"] = agg_tops.apply(lambda r: r['Topic_H'].intersection(r['shifted_H']), axis=1)\n",
    "agg_tops[\"added_H\"] = agg_tops.apply(lambda r: r['Topic_H'].difference(r['shifted_H']), axis=1)\n",
    "agg_tops[\"removed_H\"] = agg_tops.apply(lambda r: r['shifted_H'].difference(r['Topic_H']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_data = agg_tops[[\n",
    "    'CIK', 'filing_dt', 'report_dt', 'rprt_length', 'SIC', 'Industry', 'category'\n",
    "]].copy()\n",
    "\n",
    "# For micro-topics\n",
    "stat_data[\"reported_crnt\"] = agg_tops['Topic'].map(len)\n",
    "stat_data[\"reported_last\"] = agg_tops['shifted'].map(len)\n",
    "stat_data[\"repeated\"] = agg_tops[\"repeated\"].map(len)\n",
    "stat_data[\"added\"] = agg_tops[\"added\"].map(len)\n",
    "stat_data[\"removed\"] = agg_tops[\"removed\"].map(len)\n",
    "\n",
    "# For macro-topics\n",
    "stat_data[\"reported_crnt_H\"] = agg_tops['Topic_H'].map(len)\n",
    "stat_data[\"reported_last_H\"] = agg_tops['shifted_H'].map(len)\n",
    "stat_data[\"repeated_H\"] = agg_tops[\"repeated_H\"].map(len)\n",
    "stat_data[\"added_H\"] = agg_tops[\"added_H\"].map(len)\n",
    "stat_data[\"removed_H\"] = agg_tops[\"removed_H\"].map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_data['rfGap'] = (stat_data['filing_dt'] - stat_data['report_dt']).dt.days\n",
    "\n",
    "stat_data['fyear'] = stat_data['filing_dt'].dt.year\n",
    "stat_data['ryear'] = stat_data['report_dt'].dt.year\n",
    "\n",
    "# stat_data.drop_duplicates([\"CIK\", \"ryear\"], keep='first', inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables for H2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 'H2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk topics disclosed and not disclosed per report \n",
    "disc_df = pd.pivot_table(\n",
    "    topics_df, index = [\"CIK\", \"filing_dt\", \"report_dt\", \"SIC\"], \n",
    "    columns='Topic_H', values='Score_H'\n",
    ").notna().astype(int).reset_index()\n",
    "\n",
    "disc_df[\"ryear\"] = disc_df[\"report_dt\"].dt.year\n",
    "\n",
    "disc_df.sort_values(['CIK', 'filing_dt'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference between disclosed risk topics in 2 consecutive years\n",
    "disc_diff = disc_df.filter(range(0,100)) - disc_df.groupby(\"CIK\")[disc_df.filter(range(0,100)).columns].shift(1)\n",
    "\n",
    "# Number of reporting firms per fiscal year\n",
    "rprt_firms = disc_df.groupby(['ryear', 'SIC'])[\"CIK\"].transform('nunique')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To what extent the risk factors disclosed by the firm \n",
    "# are already disclosed by other firms over the past 52 weeks\n",
    "# Number of disclosing firms before the focal firm during the 52 weeks before filing date\n",
    "def count_func(x):\n",
    "    df_slice = disc_df[\n",
    "        (disc_df[\"filing_dt\"]>x[\"filing_dt\"] - pd.Timedelta(weeks=52))&\n",
    "        (disc_df[\"filing_dt\"]<x[\"filing_dt\"])&\n",
    "        (disc_df['CIK']!=x['CIK'])\n",
    "    ]\n",
    "    output = (df_slice.filter(range(0,100)).sum() + 1) / (df_slice[\"CIK\"].count() + 1)\n",
    "    \n",
    "    return output\n",
    "\n",
    "count_disc = disc_df.apply(count_func, axis=1)\n",
    "\n",
    "# count_disc.to_csv(\"count_disc.csv\")\n",
    "# count_disc = pd.read_csv(\"count_disc.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No. disclosing firms in the same industry devided by no. disclosing firms \n",
    "def count_func_ind(x):\n",
    "    df_slice = disc_df[\n",
    "        (disc_df[\"filing_dt\"]>x[\"filing_dt\"] - pd.Timedelta(weeks=52))&\n",
    "        (disc_df[\"filing_dt\"]<x[\"filing_dt\"])&\n",
    "        (disc_df['SIC']==x['SIC'])&\n",
    "        (disc_df['CIK']!=x['CIK'])\n",
    "    ]\n",
    "\n",
    "    output = (df_slice.filter(range(0,100)).sum() + 1) / (df_slice[\"CIK\"].count() + 1)\n",
    "\n",
    "    return output\n",
    "\n",
    "count_disc_ind = disc_df.apply(count_func_ind, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted disclosures\n",
    "disc_diff_w = disc_diff.multiply(count_disc.values)\n",
    "disc_w = disc_df.filter(range(0,101)).multiply(count_disc.values)\n",
    "\n",
    "disc_diff_w_ind = disc_diff.multiply(count_disc_ind.values)\n",
    "disc_w_ind = disc_df.filter(range(0,101)).multiply(count_disc_ind.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_df.drop(columns=['report_dt'], inplace=True)\n",
    "\n",
    "disc_df[\"avg_topic\"] = disc_w.mean(axis=1, skipna=True)\n",
    "# disc_df[\"std_topic\"] = disc_w.std(axis=1, skipna=True)\n",
    "\n",
    "disc_df[\"avg_added\"] = disc_diff_w[disc_diff_w>0].mean(axis=1, skipna=True)\n",
    "# disc_df[\"std_added\"] = disc_diff_w[disc_diff_w>0].std(axis=1, skipna=True)\n",
    "\n",
    "disc_df[\"avg_removed\"] = disc_diff_w[disc_diff_w<0].mean(axis=1, skipna=True)\n",
    "# disc_df[\"std_removed\"] = disc_diff_w[disc_diff_w<0].std(axis=1, skipna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_df[\"avg_topic_ind\"] = disc_w_ind.mean(axis=1, skipna=True)\n",
    "\n",
    "disc_df[\"avg_added_ind\"] = disc_diff_w_ind[disc_diff_w_ind>0].mean(axis=1, skipna=True)\n",
    "\n",
    "disc_df[\"avg_removed_ind\"] = disc_diff_w_ind[disc_diff_w_ind<0].mean(axis=1, skipna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disc_df[\"std_topic_inv\"] = (1/disc_w[disc_w>0]).std(axis=1, skipna=True)\n",
    "# disc_df[\"avg_topic_inv\"] = (1/disc_w[disc_w>0]).mean(axis=1, skipna=True)\n",
    "\n",
    "# disc_df[\"std_added_inv\"] = (1/disc_diff_w[disc_diff_w>0]).std(axis=1, skipna=True)\n",
    "# disc_df[\"avg_added_inv\"] = (1/disc_diff_w[disc_diff_w>0]).mean(axis=1, skipna=True)\n",
    "\n",
    "# disc_df[\"std_removed_inv\"] = (1/disc_diff_w[disc_diff_w<0]).std(axis=1, skipna=True)\n",
    "# disc_df[\"avg_removed_inv\"] = (1/disc_diff_w[disc_diff_w<0]).mean(axis=1, skipna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disc_cnt = topics_df.groupby('topic')[\"CIK\"].nunique()\n",
    "\n",
    "# g_25 = disc_cnt[(disc_cnt<=disc_cnt.quantile(0.25))].index.to_list()\n",
    "\n",
    "# g_75 = disc_cnt[(disc_cnt>=disc_cnt.quantile(0.75))].index.to_list()\n",
    "\n",
    "# disc_df[\"avg_topic_25\"] = disc_w.filter(g_25, axis=1).mean(axis=1)\n",
    "# disc_df[\"avg_topic_75\"] = disc_w.filter(g_75, axis=1).mean(axis=1)\n",
    "\n",
    "# disc_df[\"std_topic_25\"] = disc_w.filter(g_25, axis=1).std(axis=1)\n",
    "# disc_df[\"std_topic_75\"] = disc_w.filter(g_75, axis=1).std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disc_df[disc_w.columns] = disc_w\n",
    "# disc_df.rename(columns=dict([(x, f\"topic{x}\") for x in disc_w.columns]), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Topic_H</th>\n",
       "      <th>avg_topic</th>\n",
       "      <th>avg_added</th>\n",
       "      <th>avg_removed</th>\n",
       "      <th>avg_topic_ind</th>\n",
       "      <th>avg_added_ind</th>\n",
       "      <th>avg_removed_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>35408.000000</td>\n",
       "      <td>24412.000000</td>\n",
       "      <td>23231.000000</td>\n",
       "      <td>35408.000000</td>\n",
       "      <td>24412.000000</td>\n",
       "      <td>23231.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.064165</td>\n",
       "      <td>0.269367</td>\n",
       "      <td>-0.257724</td>\n",
       "      <td>0.091576</td>\n",
       "      <td>0.328394</td>\n",
       "      <td>-0.317904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.030513</td>\n",
       "      <td>0.108579</td>\n",
       "      <td>0.098264</td>\n",
       "      <td>0.042991</td>\n",
       "      <td>0.153171</td>\n",
       "      <td>0.153935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.012540</td>\n",
       "      <td>-0.939983</td>\n",
       "      <td>0.002011</td>\n",
       "      <td>0.005137</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.041666</td>\n",
       "      <td>0.197876</td>\n",
       "      <td>-0.312541</td>\n",
       "      <td>0.059712</td>\n",
       "      <td>0.223674</td>\n",
       "      <td>-0.403911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.059608</td>\n",
       "      <td>0.256215</td>\n",
       "      <td>-0.248960</td>\n",
       "      <td>0.085206</td>\n",
       "      <td>0.314815</td>\n",
       "      <td>-0.305043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.082505</td>\n",
       "      <td>0.322588</td>\n",
       "      <td>-0.188750</td>\n",
       "      <td>0.116932</td>\n",
       "      <td>0.415385</td>\n",
       "      <td>-0.212108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.183031</td>\n",
       "      <td>0.943875</td>\n",
       "      <td>-0.012540</td>\n",
       "      <td>0.442105</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.004673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Topic_H     avg_topic     avg_added   avg_removed  avg_topic_ind  \\\n",
       "count    35408.000000  24412.000000  23231.000000   35408.000000   \n",
       "mean         0.064165      0.269367     -0.257724       0.091576   \n",
       "std          0.030513      0.108579      0.098264       0.042991   \n",
       "min          0.000443      0.012540     -0.939983       0.002011   \n",
       "25%          0.041666      0.197876     -0.312541       0.059712   \n",
       "50%          0.059608      0.256215     -0.248960       0.085206   \n",
       "75%          0.082505      0.322588     -0.188750       0.116932   \n",
       "max          0.183031      0.943875     -0.012540       0.442105   \n",
       "\n",
       "Topic_H  avg_added_ind  avg_removed_ind  \n",
       "count     24412.000000     23231.000000  \n",
       "mean          0.328394        -0.317904  \n",
       "std           0.153171         0.153935  \n",
       "min           0.005137        -1.000000  \n",
       "25%           0.223674        -0.403911  \n",
       "50%           0.314815        -0.305043  \n",
       "75%           0.415385        -0.212108  \n",
       "max           1.000000        -0.004673  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disc_df[[\"avg_topic\", \"avg_added\", \"avg_removed\", \"avg_topic_ind\", \"avg_added_ind\", \"avg_removed_ind\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EIKON prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Instrument', 'CLOSEPRICE', 'VOLUME', 'COMPANYMARKETCAP',\n",
       "       'TTLCMNSHARESOUT'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices_df = pd.read_csv(\"Data\\Prices.csv\")\n",
    "prices_df[\"Date\"] = pd.to_datetime(prices_df[\"Date\"])\n",
    "prices_df.set_index('Date', inplace=True)\n",
    "prices_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily shares turnover\n",
    "prices_df[\"SHRTURN\"] = prices_df[\"VOLUME\"] / prices_df[\"TTLCMNSHARESOUT\"]\n",
    "\n",
    "# Returns\n",
    "prices_df[\"Return\"] = prices_df.groupby(\"Instrument\")[\"CLOSEPRICE\"].pct_change(1)\n",
    "\n",
    "prices_df[\"log_Return\"] = np.log(prices_df[\"Return\"] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time window\n",
    "N = 10\n",
    "\n",
    "# Average of N-day std of daily returns\n",
    "std_returns = (\n",
    "    prices_df.groupby(\"Instrument\")[\"log_Return\"]\n",
    "    .rolling(N, min_periods=N//2).std().to_frame()\n",
    ")\n",
    "std_returns[f\"stdReturn+{N}\"] = std_returns.groupby(\"Instrument\")[\"log_Return\"].shift(-N-2)\n",
    "std_returns[f\"stdReturn_{N}\"] = std_returns.groupby(\"Instrument\")[\"log_Return\"].shift(2)\n",
    "\n",
    "std_returns.drop(columns=\"log_Return\", inplace=True)\n",
    "\n",
    "# Average of N-day trade volumes\n",
    "MA_vol = prices_df.groupby(\"Instrument\")[\"VOLUME\"].rolling(N, min_periods=N//2).mean().to_frame()\n",
    "MA_vol[f\"VOLUME_{N}\"] = MA_vol.groupby(\"Instrument\")[\"VOLUME\"].shift(2)\n",
    "MA_vol[\"SHRTURN\"] = prices_df.groupby(\"Instrument\")[\"SHRTURN\"].rolling(N, min_periods=N//2).mean()\n",
    "\n",
    "MA_vol.drop(columns=\"VOLUME\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time window\n",
    "N = 20\n",
    "\n",
    "# Average of N-day std of daily returns\n",
    "std_returns[f\"stdReturn+{N}\"] = prices_df.groupby(\"Instrument\")[\"log_Return\"].rolling(N, min_periods=N//2).std().groupby(\"Instrument\").shift(-N-2)\n",
    "std_returns[f\"stdReturn_{N}\"] = prices_df.groupby(\"Instrument\")[\"log_Return\"].rolling(N, min_periods=N//2).std().groupby(\"Instrument\").shift(2)\n",
    "\n",
    "# Average of daily trade returns\n",
    "# MA_vol[f\"VOLUME_{N}\"] = grouped_prices[\"VOLUME\"].rolling(N).mean().groupby(\"Instrument\").shift(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time window\n",
    "N = 30\n",
    "\n",
    "# Average of 60-day std of daily returns\n",
    "std_returns[f\"stdReturn+{N}\"] = prices_df.groupby(\"Instrument\")[\"log_Return\"].rolling(N, min_periods=N//2).std().groupby(\"Instrument\").shift(-N-2)\n",
    "std_returns[f\"stdReturn_{N}\"] = prices_df.groupby(\"Instrument\")[\"log_Return\"].rolling(N, min_periods=N//2).std().groupby(\"Instrument\").shift(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bid-Ask spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 8687523 entries, 2006-01-17 to 2022-06-14\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Dtype  \n",
      "---  ------      -----  \n",
      " 0   Instrument  int64  \n",
      " 1   HIGHPRICE   float64\n",
      " 2   LOWPRICE    float64\n",
      " 3   BIDPRICE    float64\n",
      " 4   ASKPRICE    float64\n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 397.7 MB\n"
     ]
    }
   ],
   "source": [
    "BidAsk_df = pd.read_csv(\"Data\\BidAsk.csv\").drop_duplicates()\n",
    "BidAsk_df[\"Date\"] = pd.to_datetime(BidAsk_df[\"Date\"])\n",
    "BidAsk_df.set_index('Date', inplace=True)\n",
    "BidAsk_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "BidAsk_df[\"BAspread\"] = (BidAsk_df[\"ASKPRICE\"] - BidAsk_df['BIDPRICE'])/BidAsk_df[\"ASKPRICE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "\n",
    "# N-day moving average (trading days only)\n",
    "MA_BA = BidAsk_df.groupby('Instrument')['BAspread'].rolling(N, min_periods=N//2).mean().to_frame()\n",
    "MA_BA[f\"avgBA+{N}\"] = MA_BA.groupby(\"Instrument\")[\"BAspread\"].shift(-N-2)\n",
    "MA_BA[f\"avgBA_{N}\"] = MA_BA.groupby(\"Instrument\")[\"BAspread\"].shift(2)\n",
    "\n",
    "# MA_BA[f\"stdBA+{N}\"] = grouped_BA['BAspread'].rolling(N).std().groupby(\"Instrument\").shift(-N-2)\n",
    "# MA_BA[f\"stdBA_{N}\"] = grouped_BA['BAspread'].rolling(N).std().groupby(\"Instrument\").shift(2)\n",
    "\n",
    "MA_BA.drop(columns='BAspread', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "# N-day moving average (trading days only)\n",
    "MA_BA[f\"avgBA+{N}\"] = BidAsk_df.groupby('Instrument')['BAspread'].rolling(N, min_periods=N//2).mean().groupby(\"Instrument\").shift(-N-2)\n",
    "MA_BA[f\"avgBA_{N}\"] = BidAsk_df.groupby('Instrument')['BAspread'].rolling(N, min_periods=N//2).mean().groupby(\"Instrument\").shift(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 30\n",
    "# N-day moving average (trading days only)\n",
    "MA_BA[f\"avgBA+{N}\"] = BidAsk_df.groupby('Instrument')['BAspread'].rolling(N, min_periods=N//2).mean().groupby(\"Instrument\").shift(-N-2)\n",
    "MA_BA[f\"avgBA_{N}\"] = BidAsk_df.groupby('Instrument')['BAspread'].rolling(N, min_periods=N//2).mean().groupby(\"Instrument\").shift(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Instrument', 'Beta_30', 'Beta_90', 'Beta_250', 'AR', 'CAR_10',\n",
       "       'CAR_20', 'CAR_30'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Beta = pd.read_csv(\"Data\\Beta_AR.csv\")\n",
    "Beta.drop_duplicates(inplace=True)\n",
    "Beta.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Beta['Date'] = pd.to_datetime(Beta['Date'])\n",
    "Beta.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily Beta\n",
    "Beta[\"BETA+30\"] = Beta.groupby('Instrument')[\"Beta_30\"].shift(-30-2)\n",
    "Beta[\"BETA_30\"] = Beta.groupby('Instrument')[\"Beta_30\"].shift(2)\n",
    "\n",
    "Beta[\"BETA+90\"] = Beta.groupby('Instrument')[\"Beta_90\"].shift(-90-2)\n",
    "Beta[\"BETA_90\"] = Beta.groupby('Instrument')[\"Beta_90\"].shift(2)\n",
    "\n",
    "Beta[\"BETA+250\"] = Beta.groupby('Instrument')[\"Beta_250\"].shift(-250-2)\n",
    "Beta[\"BETA_250\"] = Beta.groupby('Instrument')[\"Beta_250\"].shift(2)\n",
    "\n",
    "# Daily Cost of Capital\n",
    "# Beta[\"WACC\"] = Beta.groupby('Instrument')[\"TR.WACC\"].shift(-2)\n",
    "# Beta[\"WACC_\"] = Beta.groupby('Instrument')[\"TR.WACC\"].shift(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Instrument', 'TR.NUMBEROFANALYSTS(PERIOD=FY1,METHODOLOGY=INTERIMSUM)',\n",
       "       'TR.NUMBEROFANALYSTS(PERIOD=FY1,METHODOLOGY=INTERIMSUM).DATE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Analysts = pd.read_csv(\"Data\\Analysts.csv\")\n",
    "Analysts.dropna(inplace=True)\n",
    "Analysts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Analysts.columns = ['Instrument', 'NUMBEROFANALYSTS', 'Date']\n",
    "Analysts['Date'] = pd.to_datetime(Analysts['Date'], errors='coerce').dt.tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "Analyst_df = pd.merge(\n",
    "    left=prices_df[\"Instrument\"], \n",
    "    right=Analysts, \n",
    "    on=[\"Date\", \"Instrument\"],\n",
    "    how=\"outer\"\n",
    ")\n",
    "\n",
    "Analyst_df.sort_values(['Date', 'Instrument', 'NUMBEROFANALYSTS'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Analyst_df[\"NUMBEROFANALYSTS\"] = Analyst_df.groupby(\"Instrument\")[\"NUMBEROFANALYSTS\"].fillna(method='ffill')\n",
    "Analyst_df.drop_duplicates(subset=['Instrument', 'Date'], keep='last', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Financial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annual financial data\n",
    "# Fill NAs with last (previouse) observation and drop duplicates\n",
    "financials = pd.read_csv(\n",
    "    filepath_or_buffer=\"Data\\Financials.csv\",\n",
    "    decimal=\".\", \n",
    "    thousands=',',\n",
    "    # usecols=[\n",
    "    #     'cik', 'datadate', 'act', 'at', 'dt', 'ebit', 'ebitda', 'intan', 'lct',\n",
    "    #     'lt', 'ni', 'revt', 'seq', 'teq', 'xrd', 'xt', 'mkvalt'\n",
    "    # ]\n",
    ").drop_duplicates()\n",
    "\n",
    "financials.sort_values(['cik', 'datadate'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "financials[\"datadate\"] = pd.to_datetime(financials[\"datadate\"], format=\"%d/%m/%Y\")\n",
    "financials[\"seq\"].fillna(financials[\"teq\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leverage\n",
    "financials[\"DtA\"] = financials[\"dt\"] / financials[\"at\"].replace(0, np.nan)\n",
    "financials[\"DtEBITDA\"] = financials[\"dt\"] / financials[\"ebitda\"].replace(0, np.nan)\n",
    "\n",
    "# Profitability\n",
    "financials[\"ROE\"] = financials[\"ni\"] / financials[\"seq\"].replace(0, np.nan)\n",
    "financials[\"NPM\"] = financials[\"ni\"] / financials[\"revt\"].replace(0, np.nan) # net profit margin\n",
    "\n",
    "# Firm size\n",
    "financials[\"logMC\"] = np.log(financials[\"mkvalt\"].replace(0, np.nan))\n",
    "financials[\"logTA\"] = np.log(financials[\"at\"].replace(0, np.nan))\n",
    "\n",
    "# Intangible assets\n",
    "financials[\"RtINT\"] = financials[\"revt\"] / financials[\"intan\"].replace(0, np.nan)\n",
    "financials[\"INTtA\"] = financials[\"intan\"] / financials[\"at\"].replace(0, np.nan) \n",
    "\n",
    "# Liquidity\n",
    "financials[\"current\"] = financials[\"act\"] / financials[\"lct\"].replace(0, np.nan)\n",
    "\n",
    "# Other\n",
    "financials[\"TobinQ\"] = financials[\"mkvalt\"] / financials[\"at\"].replace(0, np.nan)\n",
    "financials[\"BtM\"] = financials[\"seq\"] / financials[\"mkvalt\"].replace(0, np.nan)\n",
    "\n",
    "# R&D intensity\n",
    "financials[\"RDxopr\"] = financials[\"xrd\"].fillna(0) / financials[\"xopr\"].replace(0, np.nan)\n",
    "financials[\"ProprietaryCost\"] = financials[\"xrd\"].fillna(0) / financials.groupby('cik')[\"at\"].shift(1).replace(0, np.nan)\n",
    "\n",
    "# %change in earnings\n",
    "financials[\"DEarnings\"] = financials.groupby(\"cik\")[\"ni\"].pct_change(1).replace(np.Inf, np.nan)\n",
    "financials[\"DEarnings\"].replace(-np.Inf, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "financials['naicsh'] = financials['naicsh'].fillna(financials['naics']).astype(int)\n",
    "financials['sich'] = financials['sich'].fillna(financials['sic']).astype(int)\n",
    "\n",
    "# First 3 digits of historical NAICS > Subsector\n",
    "financials['naicsh'] = financials['naicsh'].map(lambda x: str(x)[:3])\n",
    "\n",
    "# First 2 digits of historical SIC > Major sector group\n",
    "financials['sich'] = financials['sich'].map(lambda x: str(x)[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = financials['naicsh'].map(lambda x: str(x)[:2]).replace({'32': '31', '33': '31', '45': '44', '49': '48'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(ind.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['gvkey', 'datadate', 'fyear', 'indfmt', 'consol', 'popsrc', 'datafmt',\n",
       "       'curcd', 'act', 'at', 'dt', 'ebit', 'ebitda', 'intan', 'lct', 'lt',\n",
       "       'ni', 'revt', 'seq', 'teq', 'xopr', 'xrd', 'xt', 'cik', 'costat',\n",
       "       'naicsh', 'sich', 'mkvalt', 'naics', 'sic', 'DtA', 'DtEBITDA', 'ROE',\n",
       "       'NPM', 'logMC', 'logTA', 'RtINT', 'INTtA', 'current', 'TobinQ', 'BtM',\n",
       "       'RDxopr', 'ProprietaryCost', 'DEarnings'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "financials.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_data = pd.merge(\n",
    "    left=stat_data,\n",
    "    right=std_returns,\n",
    "    left_on=[\"CIK\", \"filing_dt\"],\n",
    "    right_index=True,\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "stat_data = pd.merge(\n",
    "    left=stat_data,\n",
    "    right=MA_vol,\n",
    "    left_on=[\"CIK\", \"filing_dt\"],\n",
    "    right_index=True,\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "stat_data = pd.merge(\n",
    "    left=stat_data,\n",
    "    right=MA_BA,\n",
    "    left_on=[\"CIK\", \"filing_dt\"],\n",
    "    right_index=True,\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_data = pd.merge(\n",
    "    left=stat_data,\n",
    "    right=Beta.reset_index()[[\n",
    "        'Instrument', 'Date', \n",
    "        'BETA+30', 'BETA_30', 'BETA+90', 'BETA_90', 'BETA+250', 'BETA_250',\n",
    "        'CAR_10', 'CAR_20', 'CAR_30'\n",
    "    ]],\n",
    "    left_on=[\"CIK\", \"filing_dt\"],\n",
    "    right_on=['Instrument', 'Date'],\n",
    "    how=\"left\"\n",
    ").drop(columns=['Instrument', 'Date'])\n",
    "\n",
    "stat_data = pd.merge(\n",
    "    left=stat_data,\n",
    "    right=Analyst_df[['Date', 'Instrument', 'NUMBEROFANALYSTS']],\n",
    "    left_on=[\"CIK\", \"filing_dt\"],\n",
    "    right_on=['Instrument', 'Date'],\n",
    "    how=\"left\"\n",
    ").drop(columns=['Instrument', 'Date'])\n",
    "stat_data['NUMBEROFANALYSTS'].fillna(0, inplace=True)\n",
    "\n",
    "stat_data = pd.merge(\n",
    "    left=stat_data,\n",
    "    right=financials[[\n",
    "        'cik', 'datadate', 'naicsh', 'sich', 'DtA', 'DtEBITDA', 'ROE', 'NPM', 'mkvalt', 'logMC',\n",
    "        'at', 'logTA', 'RtINT', 'INTtA', 'current', 'TobinQ', 'BtM', 'RDxopr', 'ProprietaryCost', 'DEarnings'\n",
    "    ]],\n",
    "    left_on=[\"CIK\", \"report_dt\"],\n",
    "    right_on=[\"cik\", \"datadate\"],\n",
    "    how=\"left\"\n",
    ").drop(columns=['datadate', 'cik'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For H2 and H3\n",
    "if H in ['H2', 'H3']:\n",
    "    stat_data = pd.merge(\n",
    "        left=stat_data,\n",
    "        right=disc_df[[\n",
    "            'CIK', 'filing_dt', 'ryear', 'avg_topic', 'avg_added', 'avg_removed',\n",
    "            'avg_topic_ind', 'avg_added_ind', 'avg_removed_ind',\n",
    "            # 'std_topic', 'std_added', 'std_removed',\n",
    "            # 'std_topic_inv', 'std_added_inv', 'std_removed_inv',\n",
    "            # 'avg_topic_25', 'avg_topic_75', 'std_topic_25', 'std_topic_75'\n",
    "        ]],\n",
    "        on=[\"CIK\", \"filing_dt\", \"ryear\"],\n",
    "        how=\"left\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CIK', 'rprt_length', 'SIC', 'Industry', 'category', 'reported_crnt',\n",
       "       'reported_last', 'repeated', 'added', 'removed', 'reported_crnt_H',\n",
       "       'reported_last_H', 'repeated_H', 'added_H', 'removed_H', 'rfGap',\n",
       "       'fyear', 'ryear', 'stdReturn+10', 'stdReturn_10', 'stdReturn+20',\n",
       "       'stdReturn_20', 'stdReturn+30', 'stdReturn_30', 'VOLUME_10', 'SHRTURN',\n",
       "       'avgBA+10', 'avgBA_10', 'avgBA+20', 'avgBA_20', 'avgBA+30', 'avgBA_30',\n",
       "       'BETA+30', 'BETA_30', 'BETA+90', 'BETA_90', 'BETA+250', 'BETA_250',\n",
       "       'CAR_10', 'CAR_20', 'CAR_30', 'NUMBEROFANALYSTS', 'naicsh', 'sich',\n",
       "       'DtA', 'DtEBITDA', 'ROE', 'NPM', 'mkvalt', 'logMC', 'at', 'logTA',\n",
       "       'RtINT', 'INTtA', 'current', 'TobinQ', 'BtM', 'RDxopr',\n",
       "       'ProprietaryCost', 'DEarnings', 'avg_topic', 'avg_added', 'avg_removed',\n",
       "       'avg_topic_ind', 'avg_added_ind', 'avg_removed_ind'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # NA for added/removed means no new addition or removal of RFs => fill with 0\n",
    "# stat_data[['avg_added', 'avg_removed', 'avg_added_ind', 'avg_removed_ind']] = (\n",
    "#     stat_data[['avg_added', 'avg_removed', 'avg_added_ind', 'avg_removed_ind']].fillna(0)\n",
    "# )\n",
    "\n",
    "stat_data['Industry'] = stat_data['Industry'].map(lambda x: re.sub('Office of ', '', x[0]))\n",
    "stat_data[\"category\"] = stat_data[\"category\"].map(lambda x: x[0])\n",
    "\n",
    "stat_data.drop(columns=['filing_dt', 'report_dt'], inplace=True)\n",
    "stat_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For H2 and H3\n",
    "if H in ['H2', 'H3']:\n",
    "    stat_data.dropna(subset=['avg_topic'], inplace=True)\n",
    "#     stat_data = stat_data[stat_data.groupby(\"CIK\")['ryear'].transform('count')>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27849, 66)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_data.to_csv(f'Data\\stats_data_{model}.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datadate    >>> reporting date\\\n",
    "at          >>> Total assets (m)\\\n",
    "teq (seq)   >>> Total equity\\\n",
    "lt          >>> Total liabilities\\\n",
    "dt          >>> Total debt\\\n",
    "act         >>> Total current assets\\\n",
    "lct         >>> Total current liabilities\\\n",
    "ebit        >>> Earnings Before Interest and Taxes\\\n",
    "ebitda      >>> Earnings Before Interest, Taxes, Depreciation and Amortization\\\n",
    "ni          >>> Net Income\\\n",
    "revt        >>> Total revenue\\\n",
    "mkvalt      >>> Total market value\\\n",
    "intan       >>> Intangible assets\\\n",
    "xrd         >>> R&D expenses\\\n",
    "xt          >>> Total expenses\\\n",
    "xorp        >>> Total operating expenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get CIKs per ticker and SIC\n",
    "firm_info_df = pd.read_csv(\"firm_info.csv\")\n",
    "firm_info_df[\"CIK\"] = firm_info_df[\"CIK\"].astype(int)\n",
    "\n",
    "tickers = (\n",
    "    firm_info_df.set_index([\"CIK\", \"SIC\"])['tickers']\n",
    "    .str.strip('[]').str.replace(\"'\", \"\")\n",
    "    .replace(r'^\\s*$', np.nan, regex=True)\n",
    "    .str.split(\",\").dropna()\n",
    "    .explode()\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('ETM_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2e814d5f901f2e2f1812217256e7a2386c6b3dac449e52472436b21a420d1617"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
