{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import liberaries and functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Board data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BoardEX data\n",
    "compo = pd.read_csv(\"Data/Board-Composition.csv\", parse_dates=['AnnualReportDate']).drop(columns='Ticker').drop_duplicates()\n",
    "# network = pd.read_csv(\"Data/Board-Network.csv\")\n",
    "direct = pd.read_csv(\"Data/Board-Directors.csv\")\n",
    "CoID_CIK = pd.read_csv(\"Data/CoID_CIK.csv\")\n",
    "\n",
    "committees = pd.read_csv(\n",
    "    \"Data/BoardEx_Committees.csv\", parse_dates=['AnnualReportDate']\n",
    ").drop_duplicates().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If director is in a risk committee\n",
    "committees['RiskCommittee'] = committees['CommitteeName'].str.contains(r\"risk\", case=False).astype(int)\n",
    "\n",
    "# Only directors that are in risk committee (some directors are in multiple committees - to remove duplicates)\n",
    "Risk_committee = committees.loc[\n",
    "    committees['RiskCommittee']==1,\n",
    "    ['AnnualReportDate', 'RiskCommittee', 'BoardID', 'DirectorID']\n",
    "].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "compo = pd.merge(\n",
    "    left=compo,\n",
    "    right=Risk_committee,\n",
    "    on=['AnnualReportDate', 'BoardID', 'DirectorID'],\n",
    "    how='left'\n",
    ").fillna({'RiskCommittee': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If it is an independent director\n",
    "compo['Independent'] = compo[\"RoleName\"].str.contains('Independent', case=False).astype(int)\n",
    "\n",
    "# Get year of birth from date of birth\n",
    "def DOB(bd):\n",
    "    try:\n",
    "        return int(re.findall(pattern='\\d{4}', string=bd)[-1])\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "direct[\"YOB\"] = direct[\"DOB\"].apply(DOB)\n",
    "\n",
    "# Add 'YOB' of directors to the board composition DataFrame \n",
    "compo = pd.merge(\n",
    "    left=compo,\n",
    "    right = direct[['DirectorID', 'YOB']],\n",
    "    on=\"DirectorID\",\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "compo['Year'] = pd.to_datetime(compo['AnnualReportDate']).dt.year\n",
    "\n",
    "compo['ED'] = (compo['NED']=='No').astype(int)\n",
    "\n",
    "# Total number of boards that the board member is a member\n",
    "compo['TotCurrBrd'] = compo[['TotCurrNoLstdBrd', 'TotCurrNoUnLstdBrd']].sum(axis=1)\n",
    "\n",
    "compo['Age'] = compo['Year'] - compo['YOB']\n",
    "\n",
    "compo.drop(columns=['RowType', 'BoardName'], inplace=True)\n",
    "\n",
    "compo.fillna(\n",
    "    {'NumberDirectors': compo.groupby(['BoardID', 'AnnualReportDate'])['DirectorID'].transform('nunique')},\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number of DirectorID shared with other BoardID per year\n",
    "link_df = pd.merge(\n",
    "    left=compo[['BoardID', 'DirectorID', 'Year', 'CIKCode', 'ED', 'RiskCommittee']],\n",
    "    right=compo[['DirectorID', 'Year', 'BoardID', 'CIKCode', 'ED', 'RiskCommittee']],\n",
    "    on=['DirectorID', 'Year'],\n",
    "    how='outer',\n",
    "    suffixes=[\"\", \"_lnkd\"]\n",
    ")\n",
    "\n",
    "# # if the shared Dir is ED or RiskCommittee in either one of linked firms\n",
    "# link_df['ED'] = link_df[['ED', 'ED_lnkd']].max(axis=1)\n",
    "# link_df['RiskCommittee'] = link_df[['RiskCommittee', 'RiskCommittee_lnkd']].max(axis=1)\n",
    "\n",
    "link_df.drop(columns=['ED_lnkd', 'RiskCommittee_lnkd'], inplace=True)\n",
    "\n",
    "link_df = link_df[link_df['BoardID']!=link_df['BoardID_lnkd']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Check if linked firm is financial\n",
    "sich = pd.read_csv(\n",
    "    filepath_or_buffer=\"Data/SICH.csv\",\n",
    "    usecols=['cik', 'fyear', 'sich', 'sic']\n",
    ")\n",
    "\n",
    "sich.fillna({'sich': sich['sic']}, inplace=True)\n",
    "sich = sich.drop_duplicates().dropna().astype(int).reset_index(drop=True).drop(columns=['sic'])\n",
    "\n",
    "link_df = pd.merge(\n",
    "    left=link_df,\n",
    "    right=sich,\n",
    "    left_on=['Year', 'CIKCode_lnkd'],\n",
    "    right_on=['fyear', 'cik'],\n",
    "    how='left'\n",
    ").drop(columns=['fyear', 'cik'])\n",
    "\n",
    "sic = pd.read_csv(\"Data/SIC_df.csv\")\n",
    "\n",
    "link_df = pd.merge(\n",
    "    left=link_df,\n",
    "    right=sic[['cik', 'sic']],\n",
    "    left_on=['CIKCode_lnkd'],\n",
    "    right_on=['cik'],\n",
    "    how='left'\n",
    ").drop(columns=['cik'])\n",
    "\n",
    "link_df.fillna({'sich': link_df['sic']}, inplace=True)\n",
    "\n",
    "fin_sic = [6021, 6022, 6029, 6035, 6036, 6099, 6111, 6141, 6153, 6159, 6162, 6163, 6172, 6189, 6199,\n",
    "6200, 6211, 6221, 6282, 6311, 6321, 6324, 6331, 6351, 6361, 6399, 6411]\n",
    "\n",
    "link_df['Is_fin'] = link_df['sich'].isin(fin_sic)\n",
    "\n",
    "# Link age\n",
    "link_df = link_df.sort_values(['BoardID', 'BoardID_lnkd', 'Year']).reset_index(drop=True)\n",
    "link_df['LinkTime'] = link_df.drop_duplicates(subset=['BoardID', 'BoardID_lnkd', 'Year']).groupby(['BoardID', 'BoardID_lnkd'])['Year'].cumcount()+1\n",
    "link_df['LinkTime'] = link_df['LinkTime'].ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total connections = max(ShrdDir, LnkdFirm)\n",
    "NoShrDir = (\n",
    "    link_df.groupby(['BoardID', 'Year']).agg(\n",
    "        ShrdDir=('DirectorID', 'nunique'),\n",
    "        LnkdFirm=('BoardID_lnkd', 'nunique'),\n",
    "        FinLink=('Is_fin', 'sum'),\n",
    "        LinkTime=('LinkTime', 'mean')\n",
    "    )\n",
    ")\n",
    "\n",
    "# For directors that seat at multiple other firms\n",
    "NoShrDir[[\"ShrdED\", \"ShrdRC\"]] = link_df\\\n",
    "    .drop_duplicates(subset=['BoardID', 'DirectorID', 'Year', 'ED'])\\\n",
    "        .groupby(['BoardID', 'Year'])[['ED', 'RiskCommittee']].sum()\n",
    "\n",
    "compo = pd.merge(\n",
    "    left=compo, \n",
    "    right=NoShrDir,\n",
    "    left_on=['BoardID', 'Year'],\n",
    "    right_index=True,\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "compo.fillna(dict([(c, 0) for c in [\"ShrdDir\", \"LnkdFirm\", \"ShrdED\", \"ShrdRC\", \"FinLink\"]]), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network df based on CIK\n",
    "link_df.drop(columns=['BoardID', 'BoardID_lnkd'], inplace=True)\n",
    "link_df = link_df.dropna().astype(int).reset_index(drop=True)\n",
    "\n",
    "# Number of directors shared between each pair of firms\n",
    "network_df = link_df.groupby(['Year', 'CIKCode', 'CIKCode_lnkd'])['DirectorID'].nunique().reset_index()\n",
    "\n",
    "# Inverse number of shared directors between 2 individual firms as disctance between the 2\n",
    "network_df['Distance'] = 1/network_df['DirectorID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4313de5c62406bbc64c7a32fd91252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "centrality_list = []\n",
    "Years = network_df['Year'].unique()\n",
    "\n",
    "for yr in tqdm(Years):\n",
    "    df = network_df[network_df['Year']==yr].drop(columns='Year')\n",
    "\n",
    "    G = nx.from_pandas_edgelist(\n",
    "        df=df,\n",
    "        source='CIKCode',\n",
    "        target='CIKCode_lnkd',\n",
    "        edge_attr='Distance'\n",
    "    )\n",
    "\n",
    "    degree = nx.degree_centrality(G) # fraction of direct connections\n",
    "    # closeness = nx.closeness_centrality(G, distance='Disctance')\n",
    "    # betweenness = nx.betweenness_centrality(G, weight='Distance')\n",
    "    # eigenvector = nx.eigenvector_centrality(G, weight='Distance')\n",
    "\n",
    "    df = pd.concat([\n",
    "        pd.Series(degree, name='Degree'), \n",
    "        # pd.Series(closeness, name='Closeness'), \n",
    "        # pd.Series(betweenness, name='Betweenness'),\n",
    "        # pd.Series(eigenvector, name='Eigenvector')\n",
    "    ], axis=1).reset_index().rename(columns={'index': 'CIKCode'})\n",
    "\n",
    "    df['Year'] = yr\n",
    "\n",
    "    centrality_list.append(df)\n",
    "    \n",
    "centrality_df = pd.concat(centrality_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaN CIK codes\n",
    "compo.dropna(subset=['CIKCode'], inplace=True)\n",
    "compo['CIKCode'] = compo['CIKCode'].astype(int)\n",
    "\n",
    "compo = pd.merge(\n",
    "    left=compo, \n",
    "    right=centrality_df,\n",
    "    on=['CIKCode', 'Year'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "compo.fillna({'Degree': 0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Board composition average per firm year\n",
    "compo_sum = compo.groupby(['CIKCode', 'Year'])[[\n",
    "    'TimeInCo', 'NoQuals', 'GenderRatio', 'NationalityMix', 'NumberDirectors', 'NetworkSize', \n",
    "    'TotCurrBrd', 'Age', 'ShrdDir', 'LnkdFirm', \"ShrdED\", \"ShrdRC\", 'FinLink', 'Degree', 'LinkTime'\n",
    "]].mean()\n",
    "\n",
    "compo_sum['AgeStd'] = compo.groupby(['CIKCode', 'Year'])['Age'].std()\n",
    "compo_sum['Independent'] = compo.groupby(['CIKCode', 'Year'])['Independent'].sum()\n",
    "\n",
    "compo_sum = compo_sum.reset_index().sort_values(by=['CIKCode', 'Year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk Disclosures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H: Firms disclose more RFs if they have more linked firms/shared directors\\\n",
    "H: Firms add new directors from other companies if they have more new risk factors in year t-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3245397, 22)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Topic', 'Score', 'Topic_H', 'Score_H', 'CIK', 'report_dt', 'filing_dt',\n",
       "       'rf_seq', 'ticker', 'filerCIK', 'rf_length', 'NERs', 'Pa', 'Pr', 'Fu',\n",
       "       'Sentiment', 'FOG', 'SIC', 'FF', 'ryear', 'fyear', 'rf_seq_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load RF data \n",
    "topics_df = pd.read_csv(\"Data/RDdf_T2V5.csv\", parse_dates=['report_dt', 'filing_dt'])\n",
    "# topics_df = topics_df[topics_df['Topic']!=9019]\n",
    "print(topics_df.shape)\n",
    "topics_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6896"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_df['Topic'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df['NERs'] = topics_df['NERs'].str.replace(pat=\" \", repl=\"\").str.findall(pat=r\"'(.*?)'\")\n",
    "\n",
    "NE_labels = ['PERSON', 'NORP' 'ORG', 'GPE', 'LOC', 'PRODUCT', 'EVENT', 'LAW', 'DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY']\n",
    "topics_df['Specificity'] = topics_df['NERs'].apply(lambda NERs: len([ne for ne in NERs if ne in NE_labels]))\n",
    "\n",
    "topics_df['SIC3'] = topics_df['SIC'].map(lambda x: f\"{int(x):04d}\"[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the records at the CIK-year level\n",
    "agg_tops = (\n",
    "    topics_df.groupby([\"CIK\", \"report_dt\", \"filing_dt\", \"FF\"])[\n",
    "        ['rf_length', 'SIC3', 'Specificity', 'Pa', 'Pr', 'Fu', 'Sentiment', 'FOG']\n",
    "    ]\n",
    "    .agg({\n",
    "        'rf_length': 'sum',\n",
    "        'SIC3' : 'unique',\n",
    "        'Specificity': 'sum',\n",
    "        'Pa': 'sum', 'Pr': 'sum', 'Fu': 'sum',\n",
    "        'Sentiment': 'mean', \n",
    "        'FOG': 'mean',\n",
    "    }).reset_index()\n",
    ").drop_duplicates(subset=[\"CIK\", \"filing_dt\", \"report_dt\"]).sort_values([\"CIK\", \"filing_dt\", \"report_dt\"])\n",
    "\n",
    "agg_tops['length_1'] = agg_tops.drop_duplicates(subset=['CIK', 'filing_dt']).groupby([\"CIK\"])['rf_length'].shift(1)\n",
    "agg_tops['length_1'] = agg_tops.groupby('CIK')['length_1'].ffill()\n",
    "agg_tops['Delta_length'] = agg_tops['rf_length'] - agg_tops['length_1']\n",
    "agg_tops.drop(columns='length_1', inplace=True)\n",
    "\n",
    "agg_tops[\"SIC3\"] = agg_tops[\"SIC3\"].map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk factor clusters disclosed and not disclosed per report \n",
    "disc_df = pd.pivot_table(\n",
    "    topics_df, index = [\"CIK\", \"filing_dt\", \"report_dt\", \"FF\"], \n",
    "    columns='Topic', values='Score'\n",
    ").notna().astype(int).sort_values([\"CIK\", \"filing_dt\", \"report_dt\"]).reset_index()\n",
    "\n",
    "disc_df['ryear'] = disc_df[\"report_dt\"].dt.year\n",
    "\n",
    "# Drop firm-year observations with more than 1 report in one fiscal year\n",
    "disc_df.drop_duplicates(subset=disc_df.columns.difference([\"filing_dt\", \"report_dt\"]), keep=\"first\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = topics_df['Topic'].nunique()\n",
    "# Difference between disclosed risk topics in 2 consecutive years\n",
    "disc_diff = disc_df.filter(range(N)) - disc_df.groupby(\"CIK\")[disc_df.filter(range(N)).columns].shift(1)\n",
    "\n",
    "# Repeated risk factors\n",
    "disc_repeat = (\n",
    "    disc_df.filter(range(N))\n",
    "    + disc_df.groupby(\"CIK\")[disc_df.filter(range(N)).columns].shift(1) \n",
    "    == 2\n",
    ").astype(int)\n",
    "\n",
    "# Number of added, repeated and removed individual topics\n",
    "disc_df[\"reported\"] = disc_df.filter(range(N)).sum(axis=1)\n",
    "disc_df[\"repeated\"] = disc_repeat[disc_repeat>0].sum(axis=1)\n",
    "disc_df[\"added\"] = disc_diff[disc_diff>0].sum(axis=1)\n",
    "disc_df[\"removed\"] = disc_diff[disc_diff<0].sum(axis=1)\n",
    "\n",
    "stat_data = pd.merge(\n",
    "    left=agg_tops,\n",
    "    right=disc_df[['CIK', 'filing_dt', 'report_dt', 'FF', \n",
    "                   'reported', 'repeated', 'added', 'removed']],\n",
    "    on=['CIK', 'filing_dt', 'report_dt', 'FF'],\n",
    "    how='left'\n",
    ")\n",
    "# Number of days from fiscal year end and actual filing date\n",
    "stat_data['rfGap'] = (stat_data['filing_dt'] - stat_data['report_dt']).dt.days\n",
    "\n",
    "stat_data['fyear'] = stat_data['filing_dt'].dt.year\n",
    "stat_data['ryear'] = stat_data['report_dt'].dt.year\n",
    "\n",
    "# Number of added and removed one year ahead\n",
    "stat_data[[\"added+1\", \"removed+1\"]] = stat_data.groupby('CIK')[['added', 'removed']].shift(-1)\n",
    "# Number of added and removed one year before\n",
    "stat_data[[\"added_1\", \"removed_1\"]] = stat_data.groupby('CIK')[['added', 'removed']].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Risk topics disclosed and not disclosed per report \n",
    "# disc_df = pd.pivot_table(\n",
    "#     topics_df, index = [\"CIK\", \"filing_dt\", \"report_dt\", \"FF\"], \n",
    "#     columns='Topic_H', values='Score'\n",
    "# ).notna().astype(int).reset_index()\n",
    "\n",
    "# disc_df.sort_values(['CIK', 'filing_dt', 'report_dt'], inplace=True)\n",
    "\n",
    "# disc_df['ryear'] = disc_df[\"report_dt\"].dt.year\n",
    "\n",
    "# # Drop firm-year observations with more than 1 report in one fiscal year\n",
    "# disc_df.drop_duplicates(subset=disc_df.columns.difference([\"filing_dt\", \"report_dt\"]), keep=\"first\", inplace=True)\n",
    "# disc_df[\"filing_dt-1\"] = disc_df[\"filing_dt\"] - pd.Timedelta(weeks=52)\n",
    "\n",
    "# # To what extent the risk factors disclosed by the firm \n",
    "# # are already disclosed by other firms over the past 52 weeks\n",
    "# # Number of disclosing firms before the focal firm during the 52 weeks before filing date\n",
    "\n",
    "# def count_func(x):\n",
    "#     df_slice = disc_df[\n",
    "#         (disc_df[\"filing_dt\"]>x[\"filing_dt-1\"])&\n",
    "#         (disc_df[\"filing_dt\"]<=x[\"filing_dt\"])&\n",
    "#         (disc_df['FF']==x['FF'])\n",
    "#     ]\n",
    "\n",
    "#     output = df_slice[df_slice['CIK']!=x['CIK']].filter(range(110)).sum() / df_slice[\"filing_dt\"].count()\n",
    "\n",
    "#     return output\n",
    "\n",
    "# count_disc = disc_df.apply(count_func, axis=1)\n",
    "\n",
    "# disc_w = disc_df.filter(range(110))\n",
    "\n",
    "# disc_df[\"IndDisc\"] = disc_w[disc_w>0].multiply(count_disc.values).mean(axis=1, skipna=True)\n",
    "\n",
    "# disc_df.drop(columns='filing_dt-1', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicated ryears with report at the begining of the year\n",
    "disc_df['ryear_dupd'] = disc_df.duplicated(subset=['CIK', 'ryear'], keep='last')\n",
    "\n",
    "disc_df[\"ryear-1\"] = disc_df.groupby('CIK')['ryear'].shift(1)\n",
    "\n",
    "# change ryear if duplicated and there is a gap between two report years \n",
    "disc_df['ryear'] = disc_df[['ryear_dupd', 'ryear', 'ryear-1']].apply(\n",
    "    lambda x: x['ryear']-1 if x['ryear_dupd'] and x['ryear']-1>x['ryear-1'] else x['ryear'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "disc_df = disc_df\\\n",
    "    .drop_duplicates(subset=['CIK', 'ryear'], keep='first')\\\n",
    "        .reset_index(drop=True).drop(columns=['ryear_dupd', 'ryear-1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CIK', 'VOLUME', 'TTLCMNSHARESOUT', 'CLOSEPRICE'], dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices_df = pd.read_csv(\"Data\\WRDS_prices.csv\", parse_dates=['datadate'])\n",
    "\n",
    "prices_df.rename(columns={'cik': 'CIK', 'datadate': 'filing_dt'}, inplace=True)\n",
    "prices_df = prices_df.sort_values([\"CIK\", \"filing_dt\"]).set_index('filing_dt')\n",
    "prices_df[\"TTLCMNSHARESOUT\"] = prices_df.groupby(\"CIK\")[\"TTLCMNSHARESOUT\"].ffill()\n",
    "prices_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily shares turnover\n",
    "prices_df[\"SHRTURN\"] = prices_df[\"VOLUME\"] / prices_df[\"TTLCMNSHARESOUT\"]\n",
    "\n",
    "prices_df.loc[:, \"CLOSEPRICE\"] = prices_df.groupby(\"CIK\")[\"CLOSEPRICE\"].ffill(limit=1)\n",
    "prices_df[\"Return\"] = prices_df.groupby(\"CIK\")[\"CLOSEPRICE\"].pct_change(1, fill_method=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26589112, 6)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the list of CIKs and filing date in main dataframe and prices df\n",
    "CIKdates = pd.concat(\n",
    "    [prices_df.reset_index()[[\"CIK\", \"filing_dt\"]], disc_df[[\"CIK\", \"filing_dt\"]]], \n",
    "    axis=0\n",
    ").drop_duplicates().sort_values([\"CIK\", \"filing_dt\"])\n",
    "\n",
    "prices_df.reset_index(inplace=True)\n",
    "prices_df = pd.merge(left=CIKdates, right=prices_df, on=[\"CIK\", \"filing_dt\"], how='left')\n",
    "prices_df.set_index('filing_dt', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time window\n",
    "N = 30\n",
    "\n",
    "# Average of N-day std of daily returns\n",
    "std_returns = (\n",
    "    prices_df.groupby(\"CIK\")[\"Return\"]\n",
    "    .rolling(N, min_periods=N//2).std().to_frame()\n",
    ")\n",
    "# Window [2, 32]\n",
    "std_returns[f\"Volatility+{N}\"] = std_returns.groupby(\"CIK\")[\"Return\"].shift(-N-2)\n",
    "# Window [-32, -2]\n",
    "std_returns[f\"Volatility_{N}\"] = std_returns.groupby(\"CIK\")[\"Return\"].shift(2)\n",
    "\n",
    "# Window [-5, 60]\n",
    "std_returns[f\"Volatility+60\"] = prices_df.groupby(\"CIK\")[\"Return\"].rolling(65, min_periods=30).std().groupby(\"CIK\").shift(-60)\n",
    "\n",
    "# Average of 120-day std of daily returns\n",
    "std_returns[f\"Volatility_120\"] = prices_df.groupby(\"CIK\")[\"Return\"].rolling(120, min_periods=60).std()\n",
    "\n",
    "std_returns.drop(columns=\"Return\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average of N-day trade volumes during event\n",
    "N=10\n",
    "MA_vol = prices_df.groupby(\"CIK\")[\"SHRTURN\"].rolling(N, min_periods=6).mean().to_frame()\n",
    "MA_vol[f\"SHRTURN\"] = MA_vol.groupby(\"CIK\")[\"SHRTURN\"].shift(N//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['datadate', 'CIK', 'Beta_63', 'Beta_126', 'Beta_252', 'CAR_2', 'CAR_5',\n",
       "       'CAR_10'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Beta = pd.read_csv(\"Data\\Beta_AR.csv\", parse_dates=['datadate'])\n",
    "Beta.drop_duplicates(inplace=True)\n",
    "Beta.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Beta.rename(columns={\"Instrument\": \"CIK\", \"datadate\": \"filing_dt\"}, inplace=True)\n",
    "Beta = pd.merge(left=CIKdates, right=Beta, on=[\"CIK\", \"filing_dt\"], how='left')\n",
    "Beta.set_index(['CIK', 'filing_dt'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily Beta\n",
    "Beta[\"BETA+126\"] = Beta.groupby('CIK')[\"Beta_126\"].shift(-120)\n",
    "Beta[\"BETA+63\"] = Beta.groupby('CIK')[\"Beta_63\"].shift(-60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Analysts = pd.read_csv(\"Data/Analysts.csv\", parse_dates=['Date'])\n",
    "\n",
    "Analyst_df = pd.merge(\n",
    "    left=prices_df.reset_index()[[\"filing_dt\", \"CIK\"]], \n",
    "    right=Analysts, \n",
    "    left_on=[\"filing_dt\", \"CIK\"],\n",
    "    right_on=[\"Date\", \"Instrument\"],\n",
    "    how=\"outer\"\n",
    ").drop(columns=[\"Date\", \"Instrument\"])\n",
    "\n",
    "Analyst_df.sort_values(['CIK', 'filing_dt', 'NUMBEROFANALYSTS'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Analyst_df[\"NUMBEROFANALYSTS\"] = Analyst_df.groupby(\"CIK\")[\"NUMBEROFANALYSTS\"].ffill()\n",
    "Analyst_df.drop_duplicates(subset=[\"filing_dt\", \"CIK\"], keep='last', inplace=True)\n",
    "Analyst_df.dropna(how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fama-French industry portfolio volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns in % \n",
    "FF_rtrn = pd.read_csv(\"FF_Industry_Portfolios_daily/IndustryDaily.csv\", parse_dates=[\"Date\"], index_col=['Date'])\n",
    "\n",
    "# STDs in % - Do not *100 in stata\n",
    "FF_vol = FF_rtrn.rolling(126).std().dropna().unstack().reset_index() \n",
    "FF_vol.rename(columns={\"level_0\": \"FF\", \"level_1\": \"Date\", 0: \"IndVol_\"}, inplace=True)\n",
    "FF_vol[\"FF\"] = FF_vol[\"FF\"].astype(int)\n",
    "\n",
    "# Foreward looking IndVol\n",
    "FF_vol[\"IndVol+\"] = FF_vol.groupby(\"FF\")['IndVol_'].shift(-126)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internal control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICW = pd.read_csv(\"Data/ICW.csv\", parse_dates=['FYE_IC_OP', 'FILE_DATE'])\n",
    "\n",
    "ICW.sort_values(['COMPANY_FKEY', 'FILE_DATE'], inplace=True)\n",
    "\n",
    "ICW[\"OPyr\"] = ICW['FYE_IC_OP'].dt.year\n",
    "ICW[\"fyear\"] = ICW['FILE_DATE'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICW['Big4'] = ICW['OP_AUD_NAME'].str.contains(r'Deloitte|KPMG|Ernst|Pricewaterhouse', case=False).astype(int)\n",
    "\n",
    "ICW_gr = ICW.groupby(['COMPANY_FKEY', 'FILE_DATE'])[['COUNT_WEAK', 'Big4']].max().reset_index()\n",
    "ICW_gr2 = ICW.groupby(['COMPANY_FKEY', 'fyear'])[['COUNT_WEAK', 'Big4']].max().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_data = pd.merge(\n",
    "    left=stat_data,\n",
    "    right=ICW_gr,\n",
    "    left_on=['CIK', 'filing_dt'],\n",
    "    right_on=['COMPANY_FKEY', 'FILE_DATE'],\n",
    "    how=\"left\"\n",
    ").drop(columns=['COMPANY_FKEY', 'FILE_DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_data[['COUNT_WEAK_2', 'Big4_2']] = pd.merge(\n",
    "    left=stat_data,\n",
    "    right=ICW_gr2,\n",
    "    left_on=['CIK', 'fyear'],\n",
    "    right_on=['COMPANY_FKEY', 'fyear'],\n",
    "    how=\"left\"\n",
    ")[['COUNT_WEAK_y', 'Big4_y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_data.fillna({'COUNT_WEAK': stat_data['COUNT_WEAK_2']}, inplace=True)\n",
    "stat_data.fillna({'Big4': stat_data['Big4_2']}, inplace=True)\n",
    "\n",
    "stat_data.drop(columns=['COUNT_WEAK_2', 'Big4_2'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shares held"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Owner = pd.read_csv(\"Data\\EIKON_Ownership.csv\", parse_dates=['Date'])\n",
    "\n",
    "Invetors = [\n",
    "    'Bank and Trust', 'Corporation', 'Hedge Fund', 'Insurance Company',\n",
    "    'Investment Advisor/Hedge Fund', 'Pension Fund', 'Research Firm',\n",
    "    'Sovereign Wealth Fund', 'Venture Capital', 'Foundation',\n",
    "    'Endowment Fund', 'Holding Company', 'Independent Research Firm',\n",
    "    'Private Equity', 'Mutual Fund', 'Institution', 'Hedge Fund Portfolio',\n",
    "    'Government Agency', 'Exchange-Traded Fund', 'Brokerage Firms']\n",
    "\n",
    "Owner = Owner[Owner['Category Value'].isin(Invetors)].groupby(['Instrument', 'Date'])['Percent Of Traded Share'].sum().reset_index()\n",
    "\n",
    "Owner.rename(columns={'Percent Of Traded Share': 'InstOwnership'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Financial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "financials = pd.read_csv(\"Data\\Financials3.csv\", parse_dates=['datadate']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leverage\n",
    "financials[\"DtA\"] = financials[\"dt\"] / financials[\"at\"].replace(0, np.nan)\n",
    "\n",
    "# Profitability\n",
    "financials[\"ROE\"] = financials[\"ni\"] / financials[\"seq\"].replace(0, np.nan)\n",
    "financials[\"NPM\"] = financials[\"ni\"] / financials[\"revt\"].replace(0, np.nan) # net profit margin\n",
    "\n",
    "# Firm size\n",
    "financials[\"logMC\"] = np.log(financials[\"mkvalt\"].replace(0, np.nan))\n",
    "financials[\"logTA\"] = np.log(financials[\"at\"].replace(0, np.nan))\n",
    "\n",
    "# Intangible assets\n",
    "financials[\"INTtA\"] = financials[\"intan\"] / financials[\"at\"].replace(0, np.nan) \n",
    "\n",
    "# Liquidity\n",
    "financials[\"Current\"] = financials[\"act\"] / financials[\"lct\"].replace(0, np.nan)\n",
    "\n",
    "# Other\n",
    "financials[\"TobinQ\"] = financials[\"mkvalt\"] / financials[\"at\"].replace(0, np.nan)\n",
    "financials[\"BtM\"] = financials[\"seq\"] / financials[\"mkvalt\"].replace(0, np.nan)\n",
    "\n",
    "# R&D intensity\n",
    "financials[\"RDxopr\"] = financials[\"xrd\"].fillna(0) / financials[\"xopr\"].replace(0, np.nan)\n",
    "financials[\"ProprietaryCost\"] = financials[\"xrd\"].fillna(0) / financials.groupby('cik')[\"at\"].shift(1).replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "financials['ryear'] = financials['datadate'].dt.year\n",
    "financials['rmonth'] = financials['datadate'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "financials[\"NAs\"] = financials.isna().sum(axis=1)\n",
    "financials.sort_values([\"cik\", \"ryear\", \"rmonth\", 'NAs'], inplace=True)\n",
    "financials.drop_duplicates(subset=[\"cik\", \"ryear\", \"rmonth\"], keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stat_data = pd.merge(\n",
    "#     left=stat_data,\n",
    "#     right=disc_df[[\n",
    "#         'CIK', 'filing_dt', 'report_dt', 'IndDisc', \n",
    "#     ]],\n",
    "#     on=[\"CIK\", \"filing_dt\", \"report_dt\"],\n",
    "#     how=\"left\"\n",
    "# ).fillna({'IndDisc': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Study2_data = pd.merge(\n",
    "    left=stat_data,\n",
    "    right=compo_sum,\n",
    "    left_on=['CIK', 'ryear'],\n",
    "    right_on=['CIKCode', 'Year'],\n",
    "    how='left'\n",
    ").drop(columns=['CIKCode', 'Year'])\n",
    "\n",
    "cols = ['GenderRatio', 'NationalityMix', 'NumberDirectors', 'NetworkSize', 'TotCurrBrd', 'Age',\n",
    "       'ShrdDir', 'LnkdFirm', 'ShrdED', 'ShrdRC', 'FinLink', 'Degree', 'LinkTime', 'AgeStd', 'Independent']\n",
    "\n",
    "# Fill missing values with previous year (if any)\n",
    "Study2_data[cols] = Study2_data.groupby('CIK')[cols].ffill(limit=1)\n",
    "\n",
    "# Fill missing values with next year (if any)\n",
    "Study2_data[cols] = Study2_data.groupby('CIK')[cols].bfill(limit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "Study2_data = pd.merge(\n",
    "    left=Study2_data,\n",
    "    right=std_returns,\n",
    "    left_on=[\"CIK\", \"filing_dt\"],\n",
    "    right_index=True,\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "Study2_data = pd.merge(\n",
    "    left=Study2_data,\n",
    "    right=MA_vol,\n",
    "    left_on=[\"CIK\", \"filing_dt\"],\n",
    "    right_index=True,\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Study2_data = pd.merge(\n",
    "    left=Study2_data,\n",
    "    right=Beta.reset_index()[[\n",
    "        'CIK', 'filing_dt', 'Beta_126'\n",
    "    ]],\n",
    "    on=[\"CIK\", \"filing_dt\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "Study2_data = pd.merge(\n",
    "    left=Study2_data,\n",
    "    right=Analyst_df[[\"CIK\", \"filing_dt\", 'NUMBEROFANALYSTS']],\n",
    "    on=[\"CIK\", \"filing_dt\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "Study2_data.fillna({\"NUMBEROFANALYSTS\": 0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Study2_data[\"ryear\"] = Study2_data['report_dt'].dt.year\n",
    "Study2_data[\"rmonth\"] = Study2_data['report_dt'].dt.month\n",
    "\n",
    "fin_cols = ['DtA', 'ROE', 'NPM', 'mkvalt', 'logMC', 'at', 'logTA', 'INTtA', \n",
    "            'Current', 'TobinQ', 'BtM', 'RDxopr', 'ProprietaryCost']\n",
    "\n",
    "Study2_data = pd.merge(\n",
    "    left=Study2_data,\n",
    "    right=financials[['cik', 'ryear', 'rmonth', *fin_cols]],\n",
    "    left_on=[\"CIK\", \"ryear\", \"rmonth\"],\n",
    "    right_on=[\"cik\", \"ryear\", \"rmonth\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "financials.sort_values([\"cik\", \"ryear\", 'NAs'], inplace=True)\n",
    "financials.drop_duplicates(subset=[\"cik\", \"ryear\"], keep='first', inplace=True)\n",
    "\n",
    "df = pd.merge(\n",
    "    left=Study2_data,\n",
    "    right=financials,\n",
    "    left_on=[\"CIK\", \"ryear\"],\n",
    "    right_on=[\"cik\", \"ryear\"],\n",
    "    how=\"left\",\n",
    "    suffixes=['', '_2']\n",
    ")\n",
    "\n",
    "Study2_data.fillna(dict([(col, df[f\"{col}_2\"]) for col in fin_cols]), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Study2_data = pd.merge(\n",
    "    left=Study2_data,\n",
    "    right=FF_vol,\n",
    "    left_on=[\"FF\", \"filing_dt\"],\n",
    "    right_on=[\"FF\", \"Date\"],\n",
    "    how=\"left\"\n",
    ").drop(columns=\"Date\")\n",
    "\n",
    "Study2_data[\"fmonth\"] = Study2_data['filing_dt'].dt.month\n",
    "\n",
    "Owner['fyear'] = Owner['Date'].dt.year\n",
    "Owner['fmonth'] = Owner['Date'].dt.month\n",
    "\n",
    "Study2_data = pd.merge(\n",
    "    left=Study2_data,\n",
    "    right=Owner,\n",
    "    left_on=[\"CIK\", \"fyear\", \"fmonth\"],\n",
    "    right_on=[\"Instrument\", \"fyear\", \"fmonth\"],\n",
    "    how=\"left\"\n",
    ").drop(columns=['Instrument', 'Date', 'fmonth'])\n",
    "\n",
    "Study2_data['InstOwnership'] = Study2_data.groupby('CIK')['InstOwnership'].bfill(limit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CIK', 'report_dt', 'filing_dt', 'FF', 'rf_length', 'SIC3',\n",
       "       'Specificity', 'Pa', 'Pr', 'Fu', 'Sentiment', 'FOG', 'Delta_length',\n",
       "       'reported', 'repeated', 'added', 'removed', 'rfGap', 'fyear', 'ryear',\n",
       "       'added+1', 'removed+1', 'added_1', 'removed_1', 'COUNT_WEAK', 'Big4',\n",
       "       'TimeInCo', 'NoQuals', 'GenderRatio', 'NationalityMix',\n",
       "       'NumberDirectors', 'NetworkSize', 'TotCurrBrd', 'Age', 'ShrdDir',\n",
       "       'LnkdFirm', 'ShrdED', 'ShrdRC', 'FinLink', 'Degree', 'LinkTime',\n",
       "       'AgeStd', 'Independent', 'Volatility+30', 'Volatility_30',\n",
       "       'Volatility+60', 'Volatility_120', 'SHRTURN', 'Beta_126',\n",
       "       'NUMBEROFANALYSTS', 'rmonth', 'cik', 'DtA', 'ROE', 'NPM', 'mkvalt',\n",
       "       'logMC', 'at', 'logTA', 'INTtA', 'Current', 'TobinQ', 'BtM', 'RDxopr',\n",
       "       'ProprietaryCost', 'IndVol_', 'IndVol+', 'InstOwnership',\n",
       "       'NumberDirectors_1', 'ShrdDir_1', 'LnkdFirm_1', 'ShrdED_1', 'ShrdRC_1',\n",
       "       'Degree_1', 'AgeStd_1', 'Independent_1', 'Volatility_120_1',\n",
       "       'Beta_126_1', 'IndVol__1', 'logTA_1', 'ROE_1', 'DtA_1', 'Current_1',\n",
       "       'RDxopr_1', 'BtM_1', 'rfGap_1', 'Big4_1', 'COUNT_WEAK_1',\n",
       "       'NUMBEROFANALYSTS_1', 'reported_1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Study2_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "Study2_data.sort_values(['CIK', 'report_dt'], inplace=True)\n",
    "cols = [\n",
    "    'NumberDirectors', 'ShrdDir', 'LnkdFirm', 'ShrdED', 'ShrdRC', 'AgeStd', 'Independent', 'GenderRatio',\n",
    "    'Volatility_120', 'Beta_126', 'IndVol_', 'logTA', 'ROE', 'DtA', 'Current', 'RDxopr', 'BtM',\n",
    "    'rfGap', 'Big4', 'COUNT_WEAK', 'NUMBEROFANALYSTS', 'reported'\n",
    "]\n",
    "Study2_data[[f\"{c}_1\" for c in cols]] = Study2_data.groupby(\"CIK\")[cols].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "Study2_data.dropna(subset='Delta_length', inplace=True)\n",
    "\n",
    "Study2_data = Study2_data[Study2_data.groupby('CIK')['ryear'].transform('nunique')>1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72279, 90)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Study2_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "Study2_data.to_csv('Data/Study2_data1_V2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study2_data_V1.csv --> Interlocks identified using Board Composition data\n",
    "\n",
    "NetworkSize --> Network size of selected indivdual (number of overlaps through employment, other activities, and education)\\\n",
    "BoardID = CompanyID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Study2_data = pd.read_csv('Data/Study2_data1_V1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rc('figure', autolayout=True)\n",
    "# plt.figure(figsize=(10,10))\n",
    "# nx.draw(G, node_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdata = Study2_data.groupby('CIK')[['ShrdDir', 'reported']].mean().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdata = np.log(plotdata[plotdata['ShrdDir']>0])\n",
    "plotdata = plotdata[plotdata['reported']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(plotdata['ShrdDir'], plotdata['reported'], alpha=0.5, c='blue')\n",
    "plt.xlabel(\"Avg. Shared directors\")\n",
    "plt.ylabel(\"Avg. Risk factors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Matching Board IDs (for the focal firms) and Company IDs (for the linked firms) to corresponding CIKs\n",
    "# network = pd.merge(\n",
    "#     left=network,\n",
    "#     right=CoID_CIK,\n",
    "#     on='BoardID',\n",
    "#     how='left'\n",
    "# ).rename(columns={'CIKCode': 'CIK'})\n",
    "\n",
    "# network = pd.merge(\n",
    "#     left=network,\n",
    "#     right=CoID_CIK,\n",
    "#     left_on='CompanyID',\n",
    "#     right_on='BoardID',\n",
    "#     how='left'\n",
    "# ).drop(columns='BoardID_y').rename(columns={'CIKCode': 'LnkdCIK', 'BoardID_x': 'BoardID'})\n",
    "\n",
    "# # # Only keep associations with listed firms\n",
    "# # network = network[network[\"AssociationType\"]=='Listed Org'].reset_index(drop=True)\n",
    "\n",
    "# network[\"OverlapYearEnd\"] = network[\"OverlapYearEnd\"].replace('Curr', '2024').astype(int)\n",
    "\n",
    "# # Drop observation with missing CIKs\n",
    "# network.dropna(subset=['CIK', 'LnkdCIK'], inplace=True)\n",
    "# network[['CIK', 'LnkdCIK']] = network[['CIK', 'LnkdCIK']].astype(int)\n",
    "\n",
    "# # If the director is a brd member or not\n",
    "# network[\"NonBrd\"] = network[\"Role\"].str.contains(r\"Non-Brd\")\n",
    "# network[\"LnkdNonBrd\"] = network[\"AssociatedRole\"].str.contains(r\"Non-Brd\")\n",
    "\n",
    "# # Only keep observations that are board members either in the first or second firm\n",
    "# network.drop(\n",
    "#     network[(network['NonBrd']==True)&(network['LnkdNonBrd']==True)].index, \n",
    "#     inplace=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # What are the associations for every CIK per year\n",
    "# network_list = []\n",
    "# years = range(2005, 2024)\n",
    "# CIKs = network[\"CIK\"].unique()\n",
    "\n",
    "# for cik in tqdm(CIKs):\n",
    "#     firm = network[network['CIK']==cik]\n",
    "\n",
    "#     for yr in years:\n",
    "#         firm['Year'] = yr\n",
    "\n",
    "#         # Keep only those associations that are valid in the specific year yr\n",
    "#         firm_year = firm[(firm[\"OverlapYearStart\"]<=yr)&(firm[\"OverlapYearEnd\"]>=yr)]\n",
    "\n",
    "#         network_list.append(firm_year)\n",
    "\n",
    "# # Merge all firm_year DataFrames\n",
    "# network_df = pd.concat(network_list)\n",
    "# network_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# # Save file\n",
    "# network_df.to_csv(\"Data/Network_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network_df = pd.read_csv(\"Data/Network_df.csv\")\n",
    "\n",
    "# # Number of directors linked with other firms\n",
    "# network_df.groupby(['CIK', 'Year'])['DirectorID'].nunique()\n",
    "\n",
    "# # Number of linked firms\n",
    "# network_df.groupby(['CIK', 'Year'])['CompanyID'].nunique()\n",
    "\n",
    "# # Average number of directors shared with every other linked firm\n",
    "# network_df.groupby(['CIK', 'Year'])['CompanyID'].count()/network_df.groupby(['CIK', 'Year'])['CompanyID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PERSON:      People, including fictional.\n",
    "ORG:         Companies, agencies, institutions, etc.\n",
    "GPE:         Countries, cities, states.\n",
    "LOC:         Non-GPE locations, mountain ranges, bodies of water.\n",
    "PRODUCT:     Objects, vehicles, foods, etc. (Not services.)\n",
    "EVENT:       Named hurricanes, battles, wars, sports events, etc.\n",
    "LAW:         Named documents made into laws.\n",
    "DATE:        Absolute or relative dates or periods.\n",
    "TIME:        Times smaller than a day.\n",
    "PERCENT:     Percentage, including ”%“.\n",
    "MONEY:       Monetary values, including unit.\n",
    "QUANTITY:    Measurements, as of weight or distance.\n",
    "NORP:        Nationalities or religious or political groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('ETM_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2e814d5f901f2e2f1812217256e7a2386c6b3dac449e52472436b21a420d1617"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
