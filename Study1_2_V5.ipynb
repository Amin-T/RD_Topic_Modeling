{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55c5f8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import liberaries and functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bcffc0",
   "metadata": {},
   "source": [
    "# Risk disclosure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41cc9307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3245397, 25)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Topic', 'Score', 'Topic_H', 'Score_H', 'CIK', 'report_dt', 'filing_dt',\n",
       "       'rf_seq', 'ticker', 'filerCIK', 'rf_length', 'NERs', 'Pa', 'Pr', 'Fu',\n",
       "       'Sentiment', 'FOG', 'clean_len', 'SIC', 'FF', 'ryear', 'fyear',\n",
       "       'rf_seq_count', 'Specificity', 'SIC3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load RF data \n",
    "topics_df = pd.read_csv(\"Data/RDdf_T2V5.csv\", parse_dates=['report_dt', 'filing_dt'])\n",
    "\n",
    "topics_df['NERs'] = topics_df['NERs'].str.replace(pat=\" \", repl=\"\").str.findall(pat=r\"'(.*?)'\")\n",
    "\n",
    "NE_labels = ['PERSON', 'NORP' 'ORG', 'GPE', 'LOC', 'PRODUCT', 'EVENT', 'LAW', 'DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY']\n",
    "topics_df['Specificity'] = topics_df['NERs'].apply(lambda NERs: len([ne for ne in NERs if ne in NE_labels]))\n",
    "\n",
    "topics_df['SIC3'] = topics_df['SIC'].map(lambda x: f\"{int(x):04d}\"[:3])\n",
    "\n",
    "\n",
    "print(topics_df.shape)\n",
    "topics_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75004f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace length of raw RF with cleaned RF\n",
    "topics_df[\"rf_length\"] = topics_df[\"clean_len\"]\n",
    "topics_df.drop(columns=\"clean_len\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0783078",
   "metadata": {},
   "source": [
    "### Firm level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "121282b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = topics_df['Topic_H'].nunique()\n",
    "# Risk topics disclosed and not disclosed per report \n",
    "disc_df = pd.pivot_table(\n",
    "    topics_df, index = [\"CIK\", \"filing_dt\", \"report_dt\", \"FF\"], \n",
    "    columns='Topic_H', values='Score'\n",
    ").notna().astype(int).reset_index()\n",
    "\n",
    "# Long format\n",
    "disc_long = pd.melt(disc_df, id_vars=[\"CIK\", \"filing_dt\", \"report_dt\", \"FF\"], value_name='Disclosed')\n",
    "disc_long.sort_values([\"CIK\", 'Topic_H', \"filing_dt\", \"report_dt\"], inplace=True)\n",
    "\n",
    "disc_long['Topic_H'] = disc_long['Topic_H'].astype(int)\n",
    "\n",
    "disc_long['DiscSum'] = disc_long.groupby([\"CIK\", 'Topic_H'])['Disclosed'].cumsum()\n",
    "\n",
    "# Total number of risk topics\n",
    "disc_long['TotalRFs'] = disc_long.groupby([\"CIK\", \"filing_dt\", \"report_dt\"])['Disclosed'].transform('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84c82d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Difference between disclosed risk topics in 2 consecutive years\n",
    "disc_diff = disc_df.filter(range(N)) - disc_df.groupby(\"CIK\")[disc_df.filter(range(N)).columns].shift(1)\n",
    "\n",
    "# Repeated risk factors\n",
    "disc_repeat = (\n",
    "    disc_df.filter(range(N))\n",
    "    + disc_df.groupby(\"CIK\")[disc_df.filter(range(N)).columns].shift(1) \n",
    "    == 2\n",
    ").astype(int)\n",
    "\n",
    "# Whether risk factor was disclosed in the previouse year's report\n",
    "disc_long['LstYrDisc'] = disc_long.drop_duplicates(subset=['CIK', 'Topic_H', 'filing_dt']).groupby(['CIK', 'Topic_H'])['Disclosed'].shift(1)\n",
    "disc_long['LstYrDisc'] = disc_long.groupby(['CIK', 'Topic_H'])['LstYrDisc'].ffill()\n",
    "\n",
    "# Generate added, repeated and removed dummies\n",
    "disc_long['New'] = ((disc_long['LstYrDisc']==0)&(disc_long['Disclosed']==1)).astype(int)\n",
    "disc_long['Removed'] = ((disc_long['LstYrDisc']==1)&(disc_long['Disclosed']==0)).astype(int)\n",
    "disc_long['Repeated'] = ((disc_long['LstYrDisc']==1)&(disc_long['Disclosed']==1)).astype(int)\n",
    "\n",
    "# Whether risk factor was newly disclosed in the previouse year's report\n",
    "disc_long['LstYrNew'] = disc_long.groupby(['CIK', 'Topic_H'])['New'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b251345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep RFs that if disclosed, they are either added or repeated\n",
    "disc_long = (\n",
    "    disc_long[disc_long[['New', 'Repeated']]\n",
    "              .sum(axis=1)==disc_long['Disclosed']]\n",
    "              .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Number of days from fiscal year end and actual filing date\n",
    "disc_long['rfGap'] = (disc_long['filing_dt'] - disc_long['report_dt']).dt.days\n",
    "\n",
    "disc_long['fyear'] = disc_long[\"filing_dt\"].dt.year\n",
    "disc_long['ryear'] = disc_long[\"report_dt\"].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d3a5e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_tops = (\n",
    "    topics_df.groupby([\"CIK\", \"report_dt\", \"filing_dt\", \"Topic_H\"])[\n",
    "        ['rf_length', 'Specificity', 'Sentiment', 'FOG', 'rf_seq']\n",
    "    ]\n",
    "    .agg({\n",
    "        'rf_length': 'sum',\n",
    "        'Specificity': 'sum',\n",
    "        'Sentiment': 'mean', \n",
    "        'FOG': 'mean',\n",
    "        'rf_seq': 'mean'\n",
    "    }).reset_index()\n",
    ").drop_duplicates(subset=[\"CIK\", \"filing_dt\", \"report_dt\", \"Topic_H\"]).sort_values([\"CIK\", \"filing_dt\", \"report_dt\", \"Topic_H\"])\n",
    "\n",
    "# Length of the topics last year\n",
    "agg_tops['rf_length_1'] = agg_tops.drop_duplicates(subset=['CIK', 'Topic_H', 'filing_dt']).groupby([\"CIK\", \"Topic_H\"])['rf_length'].shift(1)\n",
    "agg_tops['rf_length_1'] = agg_tops.groupby(['CIK', 'Topic_H'])['rf_length_1'].ffill()\n",
    "\n",
    "disc_long = pd.merge(\n",
    "    left=disc_long,\n",
    "    right=agg_tops[['CIK', 'report_dt', 'filing_dt', 'Topic_H', 'rf_length', 'Specificity', \n",
    "                    'Sentiment', 'FOG', 'rf_seq', 'rf_length_1']],\n",
    "    on=['CIK', 'filing_dt', 'report_dt', 'Topic_H'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# fill NA RF attributes with 0\n",
    "disc_long.fillna(\n",
    "    {'rf_length':0, 'rf_length_1':0, 'Specificity':0, 'Sentiment':0, 'FOG':0}, \n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c3fc03",
   "metadata": {},
   "source": [
    "### Industry level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d25ab123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load filings data\n",
    "files = pd.read_csv(\n",
    "    filepath_or_buffer=\"Data/10Kurls.csv\", \n",
    "    usecols=['cik', 'periodOfReport'],\n",
    "    parse_dates=['periodOfReport']\n",
    ").dropna(subset=['cik', 'periodOfReport']).drop_duplicates().sort_values(['cik', 'periodOfReport'])\n",
    "\n",
    "# Load SIC and Fama-French data\n",
    "SIC_df = pd.read_csv(\"Data/SIC_df.csv\", usecols=['cik', 'sic']).drop_duplicates().dropna().astype(int)\n",
    "SIC_df.rename(columns={'sic': \"SIC\"}, inplace=True)\n",
    "SIC_FF = pd.read_csv(\"Data/SIC_FF.csv\")\n",
    "\n",
    "# Add SIC\n",
    "filings_df = pd.merge(\n",
    "    left=files,\n",
    "    right=SIC_df,\n",
    "    on=[\"cik\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Add Fama-French industry\n",
    "filings_df = pd.merge(\n",
    "    left=filings_df, \n",
    "    right=SIC_FF, \n",
    "    on=\"SIC\", how='left'\n",
    ").fillna({\"FF\": 49}).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1a29b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "filings_df = filings_df[\n",
    "    (filings_df[\"periodOfReport\"] >= disc_df['report_dt'].min())\n",
    "    &(filings_df[\"periodOfReport\"] <= disc_df['report_dt'].max())\n",
    "]\n",
    "filings_df['ryear'] = filings_df[\"periodOfReport\"].dt.year\n",
    "\n",
    "# Number of reporting firms per year per industry\n",
    "cnt_firms = filings_df.groupby(['FF', 'ryear'])['cik'].nunique().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f43120d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_long['FF'] = disc_long['FF'].astype(int)\n",
    "cnt_firms['FF'] = cnt_firms['FF'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "448cbcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of firms in the industry\n",
    "disc_long['#firms'] = pd.merge(\n",
    "    left=disc_long,\n",
    "    right=cnt_firms,\n",
    "    on=['FF', 'ryear'],\n",
    "    how='left'\n",
    ")['cik']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f0f05e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_df[\"filing_dt-1\"] = disc_df[\"filing_dt\"] - pd.Timedelta(weeks=52)\n",
    "\n",
    "def count_func(x):\n",
    "    \"\"\"\n",
    "    Counts the number of firms in the industry that disclose a specific RF.\n",
    "    \"\"\"\n",
    "    df_slice = disc_df[\n",
    "        (disc_df[\"filing_dt\"]>x[\"filing_dt-1\"])\n",
    "        &(disc_df[\"filing_dt\"]<=x[\"filing_dt\"])\n",
    "        &(disc_df['FF']==x['FF'])\n",
    "    ]\n",
    "    output = (\n",
    "        df_slice[df_slice['CIK']!=x['CIK']].filter(range(N)).sum() / df_slice[\"filing_dt\"].count()\n",
    "    )\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Running the function on disc_df\n",
    "IndDisc_df = disc_df.drop(columns=range(N)).copy()\n",
    "IndDisc_df.loc[:, range(N)] = disc_df.apply(count_func, axis=1)\n",
    "IndDisc_df.drop(columns=['filing_dt-1'], inplace=True)\n",
    "\n",
    "# Create the data in long format\n",
    "Inddisc_long = pd.melt(IndDisc_df, id_vars=[\"CIK\", \"filing_dt\", \"report_dt\", \"FF\"], value_name='IndDisc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59c0ff94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def other_count_func(x):\n",
    "    \"\"\"\n",
    "    Disclosing firms not in the firm's industry.\n",
    "    \"\"\"\n",
    "    df_slice = disc_df[\n",
    "        (disc_df[\"filing_dt\"]>x[\"filing_dt-1\"])&\n",
    "        (disc_df[\"filing_dt\"]<=x[\"filing_dt\"])&\n",
    "        (disc_df['FF']!=x['FF'])\n",
    "    ]\n",
    "    output = (\n",
    "        df_slice.filter(range(N)).sum() / df_slice[\"filing_dt\"].count()\n",
    "    )\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Running the function on disc_df\n",
    "OthrIndDisc_df = disc_df.drop(columns=range(N)).copy()\n",
    "OthrIndDisc_df.loc[:, range(N)] = disc_df.apply(other_count_func, axis=1)\n",
    "OthrIndDisc_df.drop(columns=['filing_dt-1'], inplace=True)\n",
    "\n",
    "# Create the data in long format\n",
    "OthrIndDisc_long = pd.melt(OthrIndDisc_df, id_vars=[\"CIK\", \"filing_dt\", \"report_dt\", \"FF\"], value_name='OtherIndDisc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a622ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop RFs that have never been disclosed per industry \n",
    "Inddisc_long = Inddisc_long[Inddisc_long.groupby(['FF', 'Topic_H'])['IndDisc'].transform('sum')>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3b9339e",
   "metadata": {},
   "outputs": [],
   "source": [
    "OthrIndDisc_long['Topic_H'] = OthrIndDisc_long['Topic_H'].astype(int)\n",
    "Inddisc_long['Topic_H'] = Inddisc_long['Topic_H'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6361db01",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_long['IndDisc'] = pd.merge(\n",
    "    left=disc_long,\n",
    "    right=Inddisc_long,\n",
    "    on=[\"CIK\", \"filing_dt\", \"report_dt\", \"Topic_H\"],\n",
    "    how='left'\n",
    ")['IndDisc'] * 100\n",
    "\n",
    "disc_long['OtherIndDisc'] = pd.merge(\n",
    "    left=disc_long,\n",
    "    right=OthrIndDisc_long,\n",
    "    on=[\"CIK\", \"filing_dt\", \"report_dt\", \"Topic_H\"],\n",
    "    how='left'\n",
    ")['OtherIndDisc'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90788ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the first year of every firm observation\n",
    "disc_long.dropna(subset=['LstYrDisc', 'IndDisc'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bccc4343",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_long.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9638a675",
   "metadata": {},
   "source": [
    "Industry specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c47da81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttest_func(x):\n",
    "    A = x['IndDisc'].values\n",
    "    B = x['OtherIndDisc'].values\n",
    "\n",
    "    tstat = stats.ttest_ind(a=A, b=B, equal_var=False).statistic\n",
    "    pvalue = stats.ttest_ind(a=A, b=B, equal_var=False).pvalue\n",
    "\n",
    "    return (tstat, pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06630711",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_df = disc_long.groupby(['FF', 'Topic_H'])[['IndDisc', 'OtherIndDisc']].mean()\n",
    "\n",
    "ttest = disc_long.groupby(['FF', 'Topic_H'])[['IndDisc', 'OtherIndDisc']].apply(ttest_func)\n",
    "\n",
    "ttest_df['tstat'] = ttest.apply(lambda x: x[0]).values\n",
    "ttest_df['pvalue'] = ttest.apply(lambda x: x[1]).values\n",
    "\n",
    "# IndDisc > NoIndDisc statistically significant at 5%\n",
    "ttest_df['IndSpecific'] = ((ttest_df['pvalue']<0.05)&(ttest_df['IndDisc']>ttest_df['OtherIndDisc'])).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6bf6c0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_long.loc[:, 'IndSpecific'] = pd.merge(\n",
    "    left=disc_long,\n",
    "    right=ttest_df,\n",
    "    how='left',\n",
    "    left_on=['FF', 'Topic_H'],\n",
    "    right_index=True\n",
    ")['IndSpecific'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28a2bf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Industry disclosure quartiles\n",
    "disc_long.loc[:, 'Qcut'] = disc_long\\\n",
    "    .groupby('Topic_H')['IndDisc']\\\n",
    "        .transform(lambda x: pd.cut(x, 4, labels=False)).astype(int) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a16fb27",
   "metadata": {},
   "source": [
    "## DVs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e8166b",
   "metadata": {},
   "source": [
    "### Volatility and BA Spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c528fe7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CIK', 'VOLUME', 'TTLCMNSHARESOUT', 'CLOSEPRICE'], dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices_df = pd.read_csv(\"Data\\Prices2.csv\", parse_dates=['datadate'])\n",
    "\n",
    "prices_df.rename(columns={'cik': 'CIK', 'datadate': 'filing_dt'}, inplace=True)\n",
    "prices_df = prices_df.sort_values([\"CIK\", \"filing_dt\"]).set_index('filing_dt')\n",
    "prices_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6fbb8ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily shares turnover\n",
    "prices_df[\"SHRTURN\"] = prices_df[\"VOLUME\"] / prices_df[\"TTLCMNSHARESOUT\"]\n",
    "\n",
    "prices_df[\"Return\"] = prices_df.groupby(\"CIK\")[\"CLOSEPRICE\"].pct_change(1, fill_method=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "212877fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the list of CIKs and filing date in main dataframe and prices df\n",
    "CIKdates = pd.concat(\n",
    "    [prices_df.reset_index()[[\"CIK\", \"filing_dt\"]], disc_df[[\"CIK\", \"filing_dt\"]]], \n",
    "    axis=0\n",
    ").drop_duplicates().sort_values([\"CIK\", \"filing_dt\"])\n",
    "\n",
    "prices_df.reset_index(inplace=True)\n",
    "prices_df = pd.merge(left=CIKdates, right=prices_df, on=[\"CIK\", \"filing_dt\"], how='left')\n",
    "prices_df.set_index('filing_dt', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f916b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time window\n",
    "N = 30\n",
    "\n",
    "# Average of N-day std of daily returns\n",
    "std_returns = (\n",
    "    prices_df.groupby(\"CIK\")[\"Return\"]\n",
    "    .rolling(N, min_periods=N//2).std().to_frame()\n",
    ")\n",
    "# Window [2, 32]\n",
    "std_returns[f\"Volatility+{N}\"] = std_returns.groupby(\"CIK\")[\"Return\"].shift(-N-2)\n",
    "# Window [-32, -2]\n",
    "std_returns[f\"Volatility_{N}\"] = std_returns.groupby(\"CIK\")[\"Return\"].shift(2)\n",
    "\n",
    "# Window [-5, 60]\n",
    "std_returns[f\"Volatility+60\"] = prices_df.groupby(\"CIK\")[\"Return\"].rolling(65, min_periods=30).std().groupby(\"CIK\").shift(-60)\n",
    "\n",
    "# Average of 120-day std of daily returns\n",
    "std_returns[f\"Volatility_120\"] = prices_df.groupby(\"CIK\")[\"Return\"].rolling(120, min_periods=60).std()\n",
    "\n",
    "std_returns.drop(columns=\"Return\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b63c44ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average of N-day trade volumes during event\n",
    "N=10\n",
    "MA_vol = prices_df.groupby(\"CIK\")[\"SHRTURN\"].rolling(N, min_periods=6).mean().to_frame()\n",
    "MA_vol[f\"SHRTURN\"] = MA_vol.groupby(\"CIK\")[\"SHRTURN\"].shift(N//2)\n",
    "\n",
    "# Window [2, 7]\n",
    "MA_vol[\"SHRTURN+5\"] = prices_df.groupby(\"CIK\")[\"SHRTURN\"].rolling(5, min_periods=3).mean().shift(-7)\n",
    "# Window [-7, -2]\n",
    "MA_vol[\"SHRTURN_5\"] = prices_df.groupby(\"CIK\")[\"SHRTURN\"].rolling(5, min_periods=3).mean().shift(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8492eae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BidAsk_df = pd.read_csv(\"Data/BidAsk2.csv\", parse_dates=['date'])\n",
    "BidAsk_df.rename(columns={\"cik\": \"CIK\", \"date\": \"filing_dt\"}, inplace=True)\n",
    "BidAsk_df = BidAsk_df[BidAsk_df[\"filing_dt\"]>'2005-01-01']\n",
    "BidAsk_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "be759200",
   "metadata": {},
   "outputs": [],
   "source": [
    "BidAsk_df[\"BAspread\"] = (BidAsk_df[\"ASKHI\"] - BidAsk_df['BIDLO'])/BidAsk_df[\"PRC\"]\n",
    "\n",
    "BidAsk_df = pd.merge(left=CIKdates, right=BidAsk_df, on=[\"CIK\", \"filing_dt\"], how='left')\n",
    "BidAsk_df.set_index('filing_dt', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "46d4d1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 30\n",
    "\n",
    "# N-day moving average (trading days only)\n",
    "MA_BA = BidAsk_df.groupby('CIK')['BAspread'].rolling(N, min_periods=N//2).mean().to_frame()\n",
    "# Window [2, 32]\n",
    "MA_BA[f\"Spread+{N}\"] = MA_BA.groupby(\"CIK\")[\"BAspread\"].shift(-N-2)\n",
    "# Window [-32, -2]\n",
    "MA_BA[f\"Spread_{N}\"] = MA_BA.groupby(\"CIK\")[\"BAspread\"].shift(2)\n",
    "\n",
    "# Window [-5, 60]\n",
    "MA_BA[\"Spread+60\"] = BidAsk_df.groupby('CIK')['BAspread'].rolling(65, min_periods=30).mean().groupby(\"CIK\").shift(-60)\n",
    "\n",
    "# Average of 120-day bid-ask spread\n",
    "MA_BA[f\"Spread_120\"] = BidAsk_df.groupby('CIK')['BAspread'].rolling(120, min_periods=60).mean()\n",
    "\n",
    "MA_BA.drop(columns='BAspread', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a253662f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PERMNO', 'DATE', 'n', 'b_mkt', 'ivol', 'tvol', 'TICKER'], dtype='object')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IVol = pd.read_csv(\"Data\\WRDS_Beta.csv\", parse_dates=['DATE'])\n",
    "IVol.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e49be4bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e3daf225",
   "metadata": {},
   "outputs": [],
   "source": [
    "CIK_PERMNO_TIC = pd.read_csv(\"Data\\CIK_PERMNO_TIC.csv\")\n",
    "\n",
    "IVol = pd.merge(\n",
    "    left=IVol,\n",
    "    right=CIK_PERMNO_TIC[['PERMNO', 'cik']],\n",
    "    on='PERMNO',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "IVol['cik2'] = pd.merge(\n",
    "    left=IVol,\n",
    "    right=CIK_PERMNO_TIC[['tic', 'cik']],\n",
    "    left_on='TICKER',\n",
    "    right_on='tic',\n",
    "    how='left'\n",
    ")['cik_y']\n",
    "\n",
    "IVol.fillna({'cik': IVol['cik2']}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5d8d7e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "IVol = IVol.drop(columns=['cik2']).dropna(subset=['cik', 'DATE'])\n",
    "\n",
    "IVol[['cik']] = IVol[['cik']].astype(int)\n",
    "IVol.rename(columns={\"cik\": \"CIK\", \"DATE\": \"filing_dt\"}, inplace=True)\n",
    "IVol = IVol.sort_values(by=[\"CIK\", \"filing_dt\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "58644c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "IVol['ivol'] = IVol['ivol'].str.replace('%', '').astype(float)\n",
    "IVol[\"Fwrd_ivol\"] = IVol.groupby(\"CIK\")[\"ivol\"].shift(-60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "beceae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the list of CIKs and filing date in main dataframe and prices df\n",
    "CIKdates = pd.concat(\n",
    "    [IVol[[\"CIK\", \"filing_dt\"]], disc_df[[\"CIK\", \"filing_dt\"]]], \n",
    "    axis=0\n",
    ").drop_duplicates().sort_values([\"CIK\", \"filing_dt\"])\n",
    "\n",
    "IVol = pd.merge(left=CIKdates, right=IVol, on=[\"CIK\", \"filing_dt\"], how='left')\n",
    "\n",
    "IVol[['b_mkt', 'ivol', 'Fwrd_ivol']] = IVol.groupby('CIK')[['b_mkt', 'ivol', 'Fwrd_ivol']].ffill(limit=3)\n",
    "\n",
    "IVol.drop(columns=['PERMNO', 'n', 'TICKER'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c51654f",
   "metadata": {},
   "source": [
    "### Earnings Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436658af",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSEst = pd.read_csv(\"Data\\EIKON_EPSforecast.csv\", parse_dates=['TR.EPSESTVALUE().DATE', 'TR.EPSESTVALUE().periodenddate'])\n",
    "EPSAct = pd.read_csv(\"Data\\EIKON_EPSActual.csv\", parse_dates=['Date', 'Report Date', 'Period End Date'])\n",
    "\n",
    "EPSEst.rename(columns={'TR.EPSESTVALUE().DATE': 'Date', \n",
    "               'TR.EPSESTVALUE().periodenddate': 'Period End Date',\n",
    "               'TR.EPSESTVALUE()': 'Earnings Per Share - Broker Estimate',\n",
    "               'TR.EPSESTVALUE().analystcode': 'Analyst Code'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "da313702",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSEst = pd.merge(\n",
    "    left=EPSEst,\n",
    "    right=EPSAct.drop(columns=['Date']),\n",
    "    on=['Instrument', 'Period End Date'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Absolute diff actual and estimated earings\n",
    "EPSEst[\"Act-Est\"] = (EPSEst['Earnings Per Share - Actual'] - EPSEst['Earnings Per Share - Broker Estimate'])\n",
    "\n",
    "EPSEst = EPSEst.sort_values(['Instrument', 'Period End Date', 'Date']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c86bf865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last fiscal year end\n",
    "FYEnd = EPSEst[['Instrument', 'Period End Date']].drop_duplicates().reset_index(drop=True)\n",
    "FYEnd['FYEnd-1'] = (\n",
    "    FYEnd.groupby(['Instrument'])['Period End Date'].shift(1)\n",
    ")\n",
    "\n",
    "EPSEst = pd.merge(\n",
    "    left=EPSEst,\n",
    "    right=FYEnd,\n",
    "    on=['Instrument', 'Period End Date'],\n",
    "    how='left'\n",
    ").dropna(subset=['FYEnd-1'])\n",
    "\n",
    "EPSEst[\"FY-1\"] = EPSEst['FYEnd-1'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "64dd7e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk disclosure dates\n",
    "disc_dts = disc_df[[\"CIK\", \"filing_dt\", \"report_dt\"]].copy()\n",
    "disc_dts['FY-1'] = disc_dts['report_dt'].dt.year\n",
    "disc_dts.drop_duplicates(subset=['CIK', 'FY-1'], inplace=True)\n",
    "\n",
    "# Match estimation errors with filing date\n",
    "EPSEst = pd.merge(\n",
    "    left=EPSEst,\n",
    "    right=disc_dts,\n",
    "    left_on=['Instrument', 'FY-1'],\n",
    "    right_on=[\"CIK\", 'FY-1'],\n",
    "    how='left'\n",
    ").dropna(subset=['CIK'])\n",
    "\n",
    "EPSEst['CIK'] = EPSEst['CIK'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8727f24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimates after filing date of last annual report\n",
    "EPSEst['After'] = (\n",
    "    (EPSEst['Date'].dt.date >= EPSEst['filing_dt']-pd.Timedelta(weeks=1))\n",
    "    & (EPSEst['Date'].dt.date < EPSEst['filing_dt']+pd.Timedelta(days=90))\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5a1fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Standasrd deviation fo analyst forecasts and Analyst forecast errors after filing date\n",
    "EPSEst_After = (\n",
    "    EPSEst[EPSEst['After']==1]\n",
    "    .groupby(['CIK', 'FY-1'])[['Earnings Per Share - Broker Estimate', 'Earnings Per Share - Actual', 'Act-Est']]\n",
    "    .agg({'Earnings Per Share - Broker Estimate': 'std', 'Act-Est': 'mean', 'Earnings Per Share - Actual': 'mean'})\n",
    ")\n",
    "EPSEst_After['nEsts'] = EPSEst[EPSEst['After']==1].groupby(['CIK', 'FY-1'])['Analyst Code'].nunique()\n",
    "EPSEst_After.rename(columns={'Earnings Per Share - Broker Estimate': 'EPSEst'}, inplace=True)\n",
    "\n",
    "EPSEst_After.reset_index(inplace=True)\n",
    "\n",
    "EPSEst['Before'] = (\n",
    "    (EPSEst['Date'].dt.date < EPSEst['filing_dt']-pd.Timedelta(weeks=1))\n",
    "    & (EPSEst['Date'].dt.date >= EPSEst['filing_dt']-pd.Timedelta(days=90))\n",
    ").astype(int)\n",
    "\n",
    "\n",
    "# Standasrd deviation fo analyst forecasts and Analyst forecast errors Before filing date\n",
    "EPSEst_Before = (\n",
    "    EPSEst[EPSEst['Before']==1]\n",
    "    .groupby(['CIK', 'FY-1'])[['Earnings Per Share - Broker Estimate', 'Act-Est']]\n",
    "    .agg({'Earnings Per Share - Broker Estimate': 'std', 'Act-Est': 'mean'})\n",
    ")\n",
    "\n",
    "EPSEst_Before.rename(columns={'Earnings Per Share - Broker Estimate': 'EPSEst'}, inplace=True)\n",
    "\n",
    "EPSEst_Before.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0df10fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSEst_After = pd.merge(\n",
    "    left=EPSEst_After,\n",
    "    right=EPSEst_Before,\n",
    "    on=['CIK', 'FY-1'],\n",
    "    how='outer',\n",
    "    suffixes=['', '_before']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e167ef0",
   "metadata": {},
   "source": [
    "## Control variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2b1d965a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['datadate', 'CIK', 'Beta_63', 'Beta_126', 'Beta_252', 'CAR_2', 'CAR_5',\n",
       "       'CAR_10'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Beta = pd.read_csv(\"Data\\Beta_AR.csv\", parse_dates=['datadate'])\n",
    "Beta.drop_duplicates(inplace=True)\n",
    "Beta.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fc1a6054",
   "metadata": {},
   "outputs": [],
   "source": [
    "Beta.rename(columns={\"Instrument\": \"CIK\", \"datadate\": \"filing_dt\"}, inplace=True)\n",
    "Beta = pd.merge(left=CIKdates, right=Beta, on=[\"CIK\", \"filing_dt\"], how='left')\n",
    "Beta.set_index(['CIK', 'filing_dt'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "eb7e0d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily Beta\n",
    "Beta[\"BETA+126\"] = Beta.groupby('CIK')[\"Beta_126\"].shift(-120)\n",
    "Beta[\"BETA+63\"] = Beta.groupby('CIK')[\"Beta_63\"].shift(-60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565af43d",
   "metadata": {},
   "source": [
    "Analysts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f6662e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "Analysts = pd.read_csv(\"Data/Analysts.csv\", parse_dates=['Date'])\n",
    "\n",
    "Analysts['Instrument'] = Analysts['Instrument'].astype(int)\n",
    "\n",
    "Analyst_df = pd.merge(\n",
    "    left=prices_df.reset_index()[[\"filing_dt\", \"CIK\"]], \n",
    "    right=Analysts, \n",
    "    left_on=[\"filing_dt\", \"CIK\"],\n",
    "    right_on=[\"Date\", \"Instrument\"],\n",
    "    how=\"outer\"\n",
    ").drop(columns=[\"Date\", \"Instrument\"])\n",
    "\n",
    "Analyst_df.sort_values(['CIK', 'filing_dt', 'NUMBEROFANALYSTS'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ab599dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Analyst_df[\"NUMBEROFANALYSTS\"] = Analyst_df.groupby(\"CIK\")[\"NUMBEROFANALYSTS\"].ffill()\n",
    "Analyst_df.drop_duplicates(subset=[\"filing_dt\", \"CIK\"], keep='last', inplace=True)\n",
    "Analyst_df.dropna(how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f11cf9",
   "metadata": {},
   "source": [
    "Fama-French industry portfolio volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55d952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns in % \n",
    "FF_rtrn = pd.read_csv(\"FF_Industry_Portfolios_daily/IndustryDaily.csv\", parse_dates=[\"Date\"], index_col=['Date'])\n",
    "\n",
    "# STDs in % - Do not *100 in stata\n",
    "FF_vol = FF_rtrn.rolling(126).std().dropna().unstack().reset_index() \n",
    "FF_vol.rename(columns={\"level_0\": \"FF\", \"level_1\": \"Date\", 0: \"IndVol_\"}, inplace=True)\n",
    "FF_vol[\"FF\"] = FF_vol[\"FF\"].astype(int)\n",
    "\n",
    "# Foreward looking IndVol\n",
    "FF_vol[\"IndVol+\"] = FF_vol.groupby(\"FF\")['IndVol_'].shift(-126)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7dd419",
   "metadata": {},
   "source": [
    "Free float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9a6a8f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FreeFloat = pd.read_csv(\"Data/FreeFloat.csv\")\n",
    "\n",
    "FreeFloat.dropna(inplace=True)\n",
    "\n",
    "FreeFloat.columns = ['CIK', 'filing_dt', 'FREEFLOAT']\n",
    "FreeFloat['filing_dt'] = pd.to_datetime(FreeFloat['filing_dt'], errors='coerce').dt.tz_localize(None)\n",
    "\n",
    "FreeFloat['CIK'] = FreeFloat['CIK'].astype(int)\n",
    "\n",
    "FreeFloat.sort_values(by=['CIK', 'filing_dt'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "105cc6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the list of CIKs and filing date in main dataframe and prices df\n",
    "CIKdates = pd.concat(\n",
    "    [FreeFloat[[\"CIK\", \"filing_dt\"]], disc_df[[\"CIK\", \"filing_dt\"]]], \n",
    "    axis=0\n",
    ").drop_duplicates().sort_values([\"CIK\", \"filing_dt\"])\n",
    "\n",
    "FreeFloat = pd.merge(left=CIKdates, right=FreeFloat, on=[\"CIK\", \"filing_dt\"], how='left')\n",
    "\n",
    "FreeFloat['FREEFLOAT'] = FreeFloat.groupby('CIK')['FREEFLOAT'].ffill(limit=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb69243",
   "metadata": {},
   "source": [
    "Financial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "42095dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "financials = pd.read_csv(\"Data\\Financials3.csv\", parse_dates=['datadate']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "54df0144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leverage\n",
    "financials[\"DtA\"] = financials[\"dt\"] / financials[\"at\"].replace(0, np.nan)\n",
    "\n",
    "# Profitability\n",
    "financials[\"ROE\"] = financials[\"ni\"] / financials[\"seq\"].replace(0, np.nan)\n",
    "financials[\"NPM\"] = financials[\"ni\"] / financials[\"revt\"].replace(0, np.nan) # net profit margin\n",
    "financials[\"ROA\"] = financials[\"ni\"] / financials[\"at\"].replace(0, np.nan) \n",
    "\n",
    "# Firm size\n",
    "financials[\"logMC\"] = np.log(financials[\"mkvalt\"].replace(0, np.nan))\n",
    "financials[\"logTA\"] = np.log(financials[\"at\"].replace(0, np.nan))\n",
    "\n",
    "# Intangible assets\n",
    "financials[\"INTtA\"] = financials[\"intan\"] / financials[\"at\"].replace(0, np.nan) \n",
    "\n",
    "# Liquidity\n",
    "financials[\"Current\"] = financials[\"act\"] / financials[\"lct\"].replace(0, np.nan)\n",
    "\n",
    "# Other\n",
    "financials[\"TobinQ\"] = financials[\"mkvalt\"] / financials[\"at\"].replace(0, np.nan)\n",
    "financials[\"BtM\"] = financials[\"seq\"] / financials[\"mkvalt\"].replace(0, np.nan)\n",
    "\n",
    "# R&D intensity\n",
    "financials[\"RDxopr\"] = financials[\"xrd\"].fillna(0) / financials[\"xopr\"].replace(0, np.nan)\n",
    "financials[\"ProprietaryCost\"] = financials[\"xrd\"].fillna(0) / financials.groupby('cik')[\"at\"].shift(1).replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d589cd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "financials['ryear'] = financials['datadate'].dt.year\n",
    "financials['rmonth'] = financials['datadate'].dt.month\n",
    "financials[\"NAs\"] = financials.isna().sum(axis=1)\n",
    "financials.sort_values([\"cik\", \"ryear\", \"rmonth\", 'NAs'], inplace=True)\n",
    "financials.drop_duplicates(subset=[\"cik\", \"ryear\", \"rmonth\"], keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "38ead7f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cik', 'datadate', 'act', 'at', 'dt', 'ebit', 'ebitda', 'intan', 'lct',\n",
       "       'lt', 'ni', 'revt', 'seq', 'teq', 'xopr', 'xrd', 'xt', 'naicsh', 'sich',\n",
       "       'mkvalt', 'naics', 'sic', 'DtA', 'ROE', 'NPM', 'ROA', 'logMC', 'logTA',\n",
       "       'INTtA', 'Current', 'TobinQ', 'BtM', 'RDxopr', 'ProprietaryCost',\n",
       "       'ryear', 'rmonth', 'NAs'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "financials.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e00e2f5",
   "metadata": {},
   "source": [
    "Shares held"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8a0628f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Owner = pd.read_csv(\"Data\\EIKON_Ownership.csv\", parse_dates=['Date'])\n",
    "\n",
    "Invetors = [\n",
    "    'Bank and Trust', 'Corporation', 'Hedge Fund', 'Insurance Company',\n",
    "    'Investment Advisor/Hedge Fund', 'Pension Fund', 'Research Firm',\n",
    "    'Sovereign Wealth Fund', 'Venture Capital', 'Foundation',\n",
    "    'Endowment Fund', 'Holding Company', 'Independent Research Firm',\n",
    "    'Private Equity', 'Mutual Fund', 'Institution', 'Hedge Fund Portfolio',\n",
    "    'Government Agency', 'Exchange-Traded Fund', 'Brokerage Firms']\n",
    "\n",
    "Owner = Owner[Owner['Category Value'].isin(Invetors)].groupby(['Instrument', 'Date'])['Percent Of Traded Share'].sum().reset_index()\n",
    "\n",
    "Owner.rename(columns={'Percent Of Traded Share': 'InstOwnership'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c6fe78",
   "metadata": {},
   "source": [
    "Internal control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "13386648",
   "metadata": {},
   "outputs": [],
   "source": [
    "ICW = pd.read_csv(\"Data/ICW.csv\", parse_dates=['FYE_IC_OP', 'FILE_DATE'])\n",
    "\n",
    "ICW.sort_values(['COMPANY_FKEY', 'FILE_DATE'], inplace=True)\n",
    "\n",
    "ICW[\"OPyr\"] = ICW['FYE_IC_OP'].dt.year\n",
    "ICW[\"fyear\"] = ICW['FILE_DATE'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a1a08bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ICW['Big4'] = ICW['OP_AUD_NAME'].str.contains(r'Deloitte|KPMG|Ernst|Pricewaterhouse', case=False).astype(int)\n",
    "\n",
    "ICW_gr = ICW.groupby(['COMPANY_FKEY', 'FILE_DATE'])[['COUNT_WEAK', 'Big4']].max().reset_index()\n",
    "ICW_gr2 = ICW.groupby(['COMPANY_FKEY', 'fyear'])[['COUNT_WEAK', 'Big4']].max().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "171a5faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_long = pd.merge(\n",
    "    left=disc_long,\n",
    "    right=ICW_gr,\n",
    "    left_on=['CIK', 'filing_dt'],\n",
    "    right_on=['COMPANY_FKEY', 'FILE_DATE'],\n",
    "    how=\"left\"\n",
    ").drop(columns=['COMPANY_FKEY', 'FILE_DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "933e9f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_long[['COUNT_WEAK_2', 'Big4_2']] = pd.merge(\n",
    "    left=disc_long,\n",
    "    right=ICW_gr2,\n",
    "    left_on=['CIK', 'fyear'],\n",
    "    right_on=['COMPANY_FKEY', 'fyear'],\n",
    "    how=\"left\"\n",
    ")[['COUNT_WEAK_y', 'Big4_y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "478c399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_long.fillna({'COUNT_WEAK': disc_long['COUNT_WEAK_2']}, inplace=True)\n",
    "disc_long.fillna({'Big4': disc_long['Big4_2']}, inplace=True)\n",
    "\n",
    "disc_long.drop(columns=['COUNT_WEAK_2', 'Big4_2'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13763649",
   "metadata": {},
   "source": [
    "## Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b642df7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Study1_data = pd.merge(\n",
    "    left=disc_long,\n",
    "    right=EPSEst_After,\n",
    "    left_on=[\"CIK\", \"ryear\"],\n",
    "    right_on=[\"CIK\", 'FY-1'],\n",
    "    how=\"left\"\n",
    ").drop(columns=['FY-1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6f91c420",
   "metadata": {},
   "outputs": [],
   "source": [
    "Study1_data = pd.merge(\n",
    "    left=Study1_data,\n",
    "    right=IVol,\n",
    "    on=[\"CIK\", \"filing_dt\"],\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "36046b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rprt_length = (\n",
    "    topics_df.groupby([\"CIK\", \"report_dt\", \"filing_dt\"])['rf_length'].sum()\n",
    ").reset_index().drop_duplicates(subset=[\"CIK\", \"filing_dt\", \"report_dt\"])\n",
    "\n",
    "rprt_length.rename(columns={'rf_length': '1A_len'}, inplace=True)\n",
    "\n",
    "Study1_data = pd.merge(\n",
    "    left=Study1_data, \n",
    "    right=rprt_length,\n",
    "    on=[\"CIK\", \"filing_dt\", \"report_dt\"],\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7399fb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "Study1_data = pd.merge(\n",
    "    left=Study1_data,\n",
    "    right=std_returns,\n",
    "    left_on=[\"CIK\", \"filing_dt\"],\n",
    "    right_index=True,\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "Study1_data = pd.merge(\n",
    "    left=Study1_data,\n",
    "    right=MA_BA,\n",
    "    left_on=[\"CIK\", \"filing_dt\"],\n",
    "    right_index=True,\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "Study1_data = pd.merge(\n",
    "    left=Study1_data,\n",
    "    right=MA_vol,\n",
    "    left_on=[\"CIK\", \"filing_dt\"],\n",
    "    right_index=True,\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9f2d686f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Study1_data = pd.merge(\n",
    "    left=Study1_data,\n",
    "    right=Beta.reset_index()[[\n",
    "        'CIK', 'filing_dt', \n",
    "        'BETA+126', 'Beta_126',\n",
    "        'CAR_5', 'CAR_2'\n",
    "    ]],\n",
    "    on=[\"CIK\", \"filing_dt\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "Study1_data = pd.merge(\n",
    "    left=Study1_data,\n",
    "    right=Analyst_df[[\"CIK\", \"filing_dt\", 'NUMBEROFANALYSTS']],\n",
    "    on=[\"CIK\", \"filing_dt\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "Study1_data.fillna({\"NUMBEROFANALYSTS\": 0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "65d31b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Study1_data = pd.merge(\n",
    "    left=Study1_data,\n",
    "    right=FF_vol,\n",
    "    left_on=[\"FF\", \"filing_dt\"],\n",
    "    right_on=[\"FF\", \"Date\"],\n",
    "    how=\"left\"\n",
    ").drop(columns=\"Date\")\n",
    "\n",
    "Study1_data[\"fmonth\"] = Study1_data['filing_dt'].dt.month\n",
    "\n",
    "Owner['fyear'] = Owner['Date'].dt.year\n",
    "Owner['fmonth'] = Owner['Date'].dt.month\n",
    "\n",
    "Study1_data = pd.merge(\n",
    "    left=Study1_data,\n",
    "    right=Owner,\n",
    "    left_on=[\"CIK\", \"fyear\", \"fmonth\"],\n",
    "    right_on=[\"Instrument\", \"fyear\", \"fmonth\"],\n",
    "    how=\"left\"\n",
    ").drop(columns=['Instrument', 'Date', 'fmonth'])\n",
    "\n",
    "Study1_data['InstOwnership'] = Study1_data.groupby('CIK')['InstOwnership'].bfill(limit=1)\n",
    "\n",
    "Study1_data = pd.merge(\n",
    "    left=Study1_data,\n",
    "    right=FreeFloat,\n",
    "    on=[\"CIK\", \"filing_dt\"],\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e7506112",
   "metadata": {},
   "outputs": [],
   "source": [
    "Study1_data[\"ryear\"] = Study1_data['report_dt'].dt.year\n",
    "Study1_data[\"rmonth\"] = Study1_data['report_dt'].dt.month\n",
    "\n",
    "fin_cols = ['DtA', 'ROE', 'NPM', 'mkvalt', 'logMC', 'at', 'logTA', 'INTtA', \n",
    "            'Current', 'TobinQ', 'BtM', 'RDxopr', 'ProprietaryCost', 'ROA']\n",
    "\n",
    "Study1_data = pd.merge(\n",
    "    left=Study1_data,\n",
    "    right=financials[['cik', 'ryear', 'rmonth', *fin_cols]],\n",
    "    left_on=[\"CIK\", \"ryear\", \"rmonth\"],\n",
    "    right_on=[\"cik\", \"ryear\", \"rmonth\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "financials.sort_values([\"cik\", \"ryear\", 'NAs'], inplace=True)\n",
    "financials.drop_duplicates(subset=[\"cik\", \"ryear\"], keep='first', inplace=True)\n",
    "\n",
    "df = pd.merge(\n",
    "    left=Study1_data,\n",
    "    right=financials,\n",
    "    left_on=[\"CIK\", \"ryear\"],\n",
    "    right_on=[\"cik\", \"ryear\"],\n",
    "    how=\"left\",\n",
    "    suffixes=['', '_2']\n",
    ")\n",
    "\n",
    "Study1_data.fillna(dict([(col, df[f\"{col}_2\"]) for col in fin_cols]), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1ec89b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7946420, 76)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Study1_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "adff37a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CIK', 'filing_dt', 'report_dt', 'FF', 'Topic_H', 'Disclosed',\n",
       "       'DiscSum', 'TotalRFs', 'LstYrDisc', 'New', 'Removed', 'Repeated',\n",
       "       'LstYrNew', 'rfGap', 'fyear', 'ryear', 'rf_length', 'Specificity',\n",
       "       'Sentiment', 'FOG', 'rf_seq', 'rf_length_1', '#firms', 'IndDisc',\n",
       "       'OtherIndDisc', 'IndSpecific', 'Qcut', 'COUNT_WEAK', 'Big4', 'EPSEst',\n",
       "       'Act-Est', 'Earnings Per Share - Actual', 'nEsts', 'EPSEst_before',\n",
       "       'Act-Est_before', 'b_mkt', 'ivol', 'tvol', 'Fwrd_ivol', '1A_len',\n",
       "       'Volatility+30', 'Volatility_30', 'Volatility+60', 'Volatility_120',\n",
       "       'Spread+30', 'Spread_30', 'Spread+60', 'Spread_120', 'SHRTURN',\n",
       "       'SHRTURN+5', 'SHRTURN_5', 'BETA+126', 'Beta_126', 'CAR_5', 'CAR_2',\n",
       "       'NUMBEROFANALYSTS', 'IndVol_', 'IndVol+', 'InstOwnership', 'FREEFLOAT',\n",
       "       'rmonth', 'cik', 'DtA', 'ROE', 'NPM', 'mkvalt', 'logMC', 'at', 'logTA',\n",
       "       'INTtA', 'Current', 'TobinQ', 'BtM', 'RDxopr', 'ProprietaryCost',\n",
       "       'ROA'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Study1_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "eb6ff109",
   "metadata": {},
   "outputs": [],
   "source": [
    "Study1_data.to_csv('Data\\study1_data2_V5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38917f91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PhD_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
