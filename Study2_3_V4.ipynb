{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import liberaries and functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BoardEX data\n",
    "compo = pd.read_csv(\"Data/Board-Composition.csv\", parse_dates=['AnnualReportDate']).drop(columns='Ticker').drop_duplicates()\n",
    "\n",
    "committees = pd.read_csv(\n",
    "    \"Data/BoardEx_Committees.csv\", parse_dates=['AnnualReportDate']\n",
    ").drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If director is in a risk committee\n",
    "committees['RiskCommittee'] = committees['CommitteeName'].str.contains(r\"risk\", case=False).astype(int)\n",
    "\n",
    "# Only directors that are in risk committee (some directors are in multiple committees - to remove duplicates)\n",
    "Risk_committee = committees.loc[\n",
    "    committees['RiskCommittee']==1,\n",
    "    ['AnnualReportDate', 'RiskCommittee', 'BoardID', 'DirectorID']\n",
    "].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "compo = pd.merge(\n",
    "    left=compo,\n",
    "    right=Risk_committee,\n",
    "    on=['AnnualReportDate', 'BoardID', 'DirectorID'],\n",
    "    how='left'\n",
    ").fillna({'RiskCommittee': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "committees['AuditCommittee'] = committees['CommitteeName'].str.contains(r\"audit\", case=False).astype(int)\n",
    "\n",
    "# Only directors that are in audit committee (some directors are in multiple committees - to remove duplicates)\n",
    "Audit_committee = committees.loc[\n",
    "    committees['AuditCommittee']==1,\n",
    "    ['AnnualReportDate', 'AuditCommittee', 'BoardID', 'DirectorID']\n",
    "].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "compo = pd.merge(\n",
    "    left=compo,\n",
    "    right=Audit_committee,\n",
    "    on=['AnnualReportDate', 'BoardID', 'DirectorID'],\n",
    "    how='left'\n",
    ").fillna({'AuditCommittee': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "compo['Year'] = pd.to_datetime(compo['AnnualReportDate']).dt.year\n",
    "\n",
    "compo['ED'] = (compo['NED']=='No').astype(int)\n",
    "\n",
    "compo['TotCurrBrd'] = compo[['TotCurrNoLstdBrd', 'TotCurrNoUnLstdBrd']].sum(axis=1)\n",
    "\n",
    "# Calculate number of DirectorID shared with other BoardID per year\n",
    "link_df = pd.merge(\n",
    "    left=compo[['BoardID', 'DirectorID', 'Year', 'CIKCode', 'ED', 'RiskCommittee', 'AuditCommittee']],\n",
    "    right=compo[['DirectorID', 'Year', 'BoardID', 'CIKCode', 'ED', 'RiskCommittee', 'AuditCommittee']],\n",
    "    on=['DirectorID', 'Year'],\n",
    "    how='outer',\n",
    "    suffixes=[\"\", \"_lnkd\"]\n",
    ")\n",
    "\n",
    "# if the shared Dir is ED or RiskCommittee in either one of linked firms\n",
    "link_df['ED'] = link_df[['ED', 'ED_lnkd']].max(axis=1)\n",
    "link_df['RiskCommittee'] = link_df[['RiskCommittee', 'RiskCommittee_lnkd']].max(axis=1)\n",
    "link_df['AuditCommittee'] = link_df[['AuditCommittee', 'AuditCommittee_lnkd']].max(axis=1)\n",
    "\n",
    "link_df.drop(columns=['ED_lnkd', 'RiskCommittee_lnkd', 'AuditCommittee_lnkd'], inplace=True)\n",
    "\n",
    "link_df = link_df[link_df['BoardID']!=link_df['BoardID_lnkd']]\n",
    "\n",
    "link_df = link_df.dropna(subset=['CIKCode', 'CIKCode_lnkd']).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Link age\n",
    "link_df = link_df.sort_values(['BoardID', 'BoardID_lnkd', 'Year']).reset_index(drop=True)\n",
    "link_df['LinkTime'] = link_df.drop_duplicates(subset=['BoardID', 'BoardID_lnkd', 'Year']).groupby(['BoardID', 'BoardID_lnkd'])['Year'].cumcount()+1\n",
    "link_df['LinkTime'] = link_df['LinkTime'].ffill()\n",
    "\n",
    "# List of linked firms\n",
    "lnkdCIKs = link_df.groupby(['CIKCode'])['CIKCode_lnkd'].agg(lambda x: list(set(x)))\n",
    "\n",
    "link_df[['CIKCode', 'CIKCode_lnkd']] = link_df[['CIKCode', 'CIKCode_lnkd']].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Firm data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CIK', 'report_dt', 'filing_dt', 'FF', 'rf_length', 'SIC3',\n",
       "       'Specificity', 'Pa', 'Pr', 'Fu', 'Sentiment', 'Delta_length',\n",
       "       'reported', 'repeated', 'added', 'removed', 'rfGap', 'fyear', 'ryear',\n",
       "       'Item1AFOG', 'added+1', 'removed+1', 'Specificity+1', 'Sentiment+1',\n",
       "       'Item1AFOG+1', 'added_1', 'removed_1', 'Specificity_1', 'Sentiment_1',\n",
       "       'Item1AFOG_1', 'COUNT_WEAK', 'Big4', 'AUDITOR_FKEY', 'GenderRatio',\n",
       "       'NationalityMix', 'NumberDirectors', 'TotCurrBrd', 'Age', 'ShrdDir',\n",
       "       'LnkdFirm', 'ShrdED', 'ShrdRC', 'ShrdAC', 'FinLink', 'LinkTime',\n",
       "       'Degree', 'Independent', 'Volatility+30', 'Volatility_30',\n",
       "       'Volatility+60', 'Volatility_120', 'SHRTURN', 'Beta_126',\n",
       "       'NUMBEROFANALYSTS', 'rmonth', 'cik', 'DtA', 'ROE', 'NPM', 'mkvalt',\n",
       "       'logMC', 'at', 'logTA', 'INTtA', 'Current', 'TobinQ', 'BtM', 'RDxopr',\n",
       "       'ProprietaryCost', 'ROA', 'IndVol_', 'IndVol+', 'InstOwnership',\n",
       "       'NumberDirectors_1', 'ShrdDir_1', 'LnkdFirm_1', 'ShrdED_1', 'ShrdRC_1',\n",
       "       'ShrdAC_1', 'Independent_1', 'GenderRatio_1', 'Volatility_120_1',\n",
       "       'Beta_126_1', 'IndVol__1', 'logTA_1', 'ROE_1', 'DtA_1', 'Current_1',\n",
       "       'RDxopr_1', 'BtM_1', 'rfGap_1', 'Big4_1', 'COUNT_WEAK_1',\n",
       "       'NUMBEROFANALYSTS_1', 'reported_1', 'SharedRF'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firm_data = pd.read_csv('Data/Study2_data1_V3.csv', parse_dates=['report_dt', 'filing_dt'])\n",
    "firm_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "firm_data.dropna(subset=[\n",
    "    'Volatility_120', 'Beta_126', 'logTA', 'ROE', 'DtA', 'Current', 'RDxopr', 'BtM',\n",
    "    'GenderRatio', 'NUMBEROFANALYSTS', 'Age', 'Independent', 'TotCurrBrd'\n",
    "], inplace=True)\n",
    "\n",
    "firm_data = firm_data[firm_data.groupby('CIK')['ryear'].transform('nunique')>1].reset_index(drop=True)\n",
    "\n",
    "match_cols = [\n",
    "    'Volatility_120', 'Beta_126', 'logTA', 'ROE', 'DtA', 'Current', 'RDxopr', 'BtM', 'COUNT_WEAK',\n",
    "    'GenderRatio', 'NUMBEROFANALYSTS', 'Age', 'Independent', 'TotCurrBrd'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45319, 96)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firm_data.fillna(dict((c,0) for c in match_cols), inplace=True)\n",
    "\n",
    "firm_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared RFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H: Firm A discloses (new) RF in year t, it connects with B in t+1 that has disclosed RF in year t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3245397, 25)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Topic', 'Score', 'Topic_H', 'Score_H', 'CIK', 'report_dt', 'filing_dt',\n",
       "       'rf_seq', 'ticker', 'filerCIK', 'rf_length', 'NERs', 'Pa', 'Pr', 'Fu',\n",
       "       'Sentiment', 'FOG', 'clean_len', 'SIC', 'FF', 'ryear', 'fyear',\n",
       "       'rf_seq_count', 'Specificity', 'SIC3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load RF data \n",
    "topics_df = pd.read_csv(\"Data/RDdf_T2V5.csv\", parse_dates=['report_dt', 'filing_dt'])\n",
    "\n",
    "topics_df['NERs'] = topics_df['NERs'].str.replace(pat=\" \", repl=\"\").str.findall(pat=r\"'(.*?)'\")\n",
    "\n",
    "NE_labels = ['PERSON', 'NORP' 'ORG', 'GPE', 'LOC', 'PRODUCT', 'EVENT', 'LAW', 'DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY']\n",
    "topics_df['Specificity'] = topics_df['NERs'].apply(lambda NERs: len([ne for ne in NERs if ne in NE_labels]))\n",
    "\n",
    "topics_df['SIC3'] = topics_df['SIC'].map(lambda x: f\"{int(x):04d}\"[:3])\n",
    "\n",
    "print(topics_df.shape)\n",
    "topics_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk topics disclosed and not disclosed per report \n",
    "disc_df = pd.pivot_table(\n",
    "    topics_df, index = [\"CIK\", \"filing_dt\", \"report_dt\"], \n",
    "    columns='Topic_H', values='Score'\n",
    ").notna().astype(int).reset_index()\n",
    "\n",
    "disc_df.sort_values(['CIK', 'filing_dt', 'report_dt'], inplace=True)\n",
    "\n",
    "disc_df['ryear'] = disc_df[\"report_dt\"].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop disclosure data with missing control variables\n",
    "disc_df = pd.merge(\n",
    "    left=disc_df,\n",
    "    right=firm_data[['CIK', 'report_dt', 'filing_dt', 'logTA']],\n",
    "    on=['CIK', 'report_dt', 'filing_dt'],\n",
    "    how='inner' # keeps only firm-year observations with control variables\n",
    ").drop(columns='logTA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop firm-year observations with more than 1 report in one fiscal year\n",
    "disc_df.drop_duplicates(subset=disc_df.columns.difference([\"filing_dt\", \"report_dt\"]), keep=\"first\", inplace=True)\n",
    "\n",
    "# duplicated ryears with report at the begining of the year\n",
    "disc_df['ryear_dupd'] = disc_df.duplicated(subset=['CIK', 'ryear'], keep='last')\n",
    "\n",
    "disc_df[\"ryear-1\"] = disc_df.groupby('CIK')['ryear'].shift(1)\n",
    "\n",
    "# change ryear if duplicated and there is a gap between two report years \n",
    "disc_df['ryear'] = disc_df[['ryear_dupd', 'ryear', 'ryear-1']].apply(\n",
    "    lambda x: x['ryear']-1 if x['ryear_dupd'] and x['ryear']-1>x['ryear-1'] else x['ryear'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "disc_df = disc_df\\\n",
    "    .drop_duplicates(subset=['CIK', 'ryear'], keep='first')\\\n",
    "        .reset_index(drop=True).drop(columns=['ryear_dupd', 'ryear-1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0245abd05d84f048c4b39afe09274b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Number of shared RFs\n",
    "Years = disc_df['ryear'].unique()\n",
    "\n",
    "shared_disc_list = []\n",
    "\n",
    "for yr in tqdm(Years):\n",
    "    df1 = disc_df[disc_df['ryear']==yr].copy()\n",
    "\n",
    "    # Caculate the number of shared RFs as the matrix multiplication of disclosure matrix by itself\n",
    "    # Only keep the upper triangle of product\n",
    "    # Add 1 to product values so only elements below diagonal is zero\n",
    "    a = df1.loc[:, range(105)].to_numpy()\n",
    "    df2 = pd.DataFrame(\n",
    "        np.triu(np.matmul(a, a.T)+1, k=1), index=df1['CIK'], columns=df1[['CIK', \"report_dt\"]]\n",
    "    ).reset_index()\n",
    "\n",
    "    df2[\"report_dt\"] = df1[\"report_dt\"].values\n",
    "\n",
    "    df3 = pd.melt(df2, id_vars=['CIK', \"report_dt\"], value_name='SharedRF')\n",
    "\n",
    "    df3 = df3[df3['SharedRF']>0]\n",
    "\n",
    "    df3['ryear'] = yr\n",
    "\n",
    "    shared_disc_list.append(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_disc_df = pd.concat(shared_disc_list)\n",
    "\n",
    "# Subtract the 1 added to np.matmul\n",
    "shared_disc_df['SharedRF'] = shared_disc_df['SharedRF'] - 1\n",
    "\n",
    "shared_disc_df['CIK_pair'] = shared_disc_df['variable'].apply(lambda x: x[0])\n",
    "shared_disc_df['report_dt_pair'] = shared_disc_df['variable'].apply(lambda x: x[1])\n",
    "\n",
    "shared_disc_df.drop(columns='variable', inplace=True)\n",
    "\n",
    "shared_disc_df = shared_disc_df[shared_disc_df['CIK']!=shared_disc_df['CIK_pair']]\n",
    "\n",
    "# Firm pairs with at least 4 years of observation for DID\n",
    "shared_disc_df = shared_disc_df[shared_disc_df.groupby(['CIK', 'CIK_pair'])['ryear'].transform('nunique')>=4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df1\n",
    "del df2\n",
    "del df3\n",
    "del shared_disc_list\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of shared Dir between 2 individual firms in a specific year\n",
    "shared_disc_df['NoSharedDir'] = pd.merge(\n",
    "    left=shared_disc_df,\n",
    "    right=link_df.groupby(['CIKCode', 'Year', 'CIKCode_lnkd'])['DirectorID'].nunique(),\n",
    "    left_on=['CIK', 'ryear', 'CIK_pair'],\n",
    "    right_index=True,\n",
    "    how='left'\n",
    ")['DirectorID']\n",
    "\n",
    "# Check if CIK pairs are linked\n",
    "shared_disc_df['Linked'] = shared_disc_df['NoSharedDir'].notna().astype(int)\n",
    "\n",
    "shared_disc_df['Treatment'] = (shared_disc_df.groupby(['CIK', 'CIK_pair'])['Linked'].transform('max')>0)\n",
    "\n",
    "# Drop always treated firms pairs\n",
    "shared_disc_df = shared_disc_df[shared_disc_df.groupby(['CIK', 'CIK_pair'])['Linked'].transform('min')==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matching procedure:**\\\n",
    "For every firm-pair AB in the treament group, we match the most similar firm to A in the last year before firms A and B share a director. Then we find the closest firm to B in that year which is never linked to firm A'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Treat_df = shared_disc_df[shared_disc_df['Linked']==1]\\\n",
    "    .sort_values(['CIK', 'CIK_pair', 'ryear'])\\\n",
    "        .reset_index(drop=True).copy()\n",
    "\n",
    "# First year 2 firms share a director\n",
    "Treat_df['eventX'] = Treat_df.groupby(['CIK', 'CIK_pair'])['ryear'].transform('min')\n",
    "\n",
    "# treated firm pairs at the first year of treatement\n",
    "treated_pairs = Treat_df[\n",
    "    (Treat_df['ryear']==Treat_df['eventX'])\n",
    "    &(Treat_df['eventX']<=2022) # and drop firms that are linked in 2023 for the first time\n",
    "][['CIK', 'report_dt', 'CIK_pair', 'report_dt_pair', 'eventX']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify All Valid Control Firm-Pairs at T−1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get top-k neighbors for a single firm\n",
    "def get_top_k_neighbors(firm_id, year, k=10):\n",
    "    year_firms = firm_data[firm_data['ryear'] == year]\n",
    "    firm_row = year_firms[year_firms['CIK'] == firm_id]\n",
    "\n",
    "    if firm_row.empty:\n",
    "        return []\n",
    "    \n",
    "    X = year_firms[match_cols].values\n",
    "    X_firm = firm_row[match_cols].values\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    X_scaled = scaler.transform(X)\n",
    "    X_firm_scaled = scaler.transform(X_firm)\n",
    "\n",
    "    knn = NearestNeighbors(n_neighbors=k+1)  # +1 to exclude the firm itself\n",
    "    knn.fit(X_scaled)\n",
    "    dists, indices = knn.kneighbors(X_firm_scaled)\n",
    "    \n",
    "    neighbors = year_firms.iloc[indices[0]]\n",
    "    neighbors = neighbors[neighbors['CIK'] != firm_id]  # Exclude self\n",
    "    return neighbors['CIK'].tolist()[:k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ad510befd14011a1f8427ab7c446de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14883 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1: Melt to long format\n",
    "firm_a = treated_pairs[['CIK', 'eventX']].rename(columns={'CIK': 'CIK'})\n",
    "firm_b = treated_pairs[['CIK_pair', 'eventX']].rename(columns={'CIK_pair': 'CIK'})\n",
    "\n",
    "# Combine and drop duplicates\n",
    "firms_for_knn = pd.concat([firm_a, firm_b], ignore_index=True)\n",
    "\n",
    "# get pre-treatment year (T−1)\n",
    "firms_for_knn['year'] = firms_for_knn['eventX'] - 1\n",
    "firms_for_knn = firms_for_knn.drop(columns='eventX').drop_duplicates()\n",
    "\n",
    "# Store results in a dictionary\n",
    "knn_dict = {}\n",
    "\n",
    "for row in tqdm(firms_for_knn.itertuples(), total=len(firms_for_knn)):\n",
    "    cik, year = row.CIK, row.year\n",
    "    neighbors = get_top_k_neighbors(cik, year, k=10)  # your earlier function\n",
    "    if neighbors:\n",
    "        knn_dict[(cik, year)] = neighbors\n",
    "    else:\n",
    "        year = year + 1\n",
    "        neighbors = get_top_k_neighbors(cik, year, k=10)\n",
    "        if neighbors:\n",
    "            knn_dict[(cik, year)] = neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_control_pairs = shared_disc_df[~shared_disc_df[\"Treatment\"]][['CIK', 'CIK_pair', 'ryear']]\\\n",
    "    .reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save treated_pairs DataFrame\n",
    "treated_pairs.to_pickle(\"treated_pairs3.pkl\")\n",
    "\n",
    "# Save initial_control_pairs DataFrame\n",
    "initial_control_pairs.to_pickle(\"initial_control_pairs3.pkl\")\n",
    "\n",
    "# Save knn_dict\n",
    "with open(\"knn_dict3.pkl\", \"wb\") as f:\n",
    "    pickle.dump(knn_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Covariates for Control Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"valid_control_pairs3.pkl\", \"rb\") as f:\n",
    "    valid_control_pairs = pickle.load(f)\n",
    "\n",
    "# Convert to DataFrame\n",
    "control_df = pd.DataFrame(valid_control_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten control pairs\n",
    "flattened_control = []\n",
    "for i, row in control_df.iterrows():\n",
    "    A, B = row.treated_pair\n",
    "    A_, B_ = row.control_pair\n",
    "    year = row.match_year\n",
    "    flattened_control.append({\n",
    "        'treated_firm1': A,\n",
    "        'treated_firm2': B,\n",
    "        'firm1': A_,\n",
    "        'firm2': B_,\n",
    "        'year': year,\n",
    "        'treated': 0\n",
    "    })\n",
    "control_df_flat = pd.DataFrame(flattened_control)\n",
    "\n",
    "# Create the Treated Sample (at T−1)\n",
    "treated_records = []\n",
    "for i, row in treated_pairs.iterrows():\n",
    "    A, B, T = row['CIK'], row['CIK_pair'], row['eventX']\n",
    "    year = max(T - 1, 2006)\n",
    "    treated_records.append({\n",
    "        'treated_firm1': A,\n",
    "        'treated_firm2': B,\n",
    "        'firm1': A,\n",
    "        'firm2': B,\n",
    "        'year': year,\n",
    "        'treated': 1\n",
    "    })\n",
    "treated_df = pd.DataFrame(treated_records)\n",
    "\n",
    "# Combine Treated + Control Samples\n",
    "pair_df = pd.concat([treated_df, control_df_flat], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge firm1 covariates\n",
    "pair_df = pair_df.merge(\n",
    "    firm_data[['CIK', 'ryear', 'FF']+match_cols].rename(columns=lambda x: f\"{x}_1\" if x not in ['CIK', 'ryear'] else x),\n",
    "    left_on=['firm1', 'year'],\n",
    "    right_on=['CIK', 'ryear'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Merge firm2 covariates\n",
    "pair_df = pair_df.merge(\n",
    "    firm_data[['CIK', 'ryear', 'FF']+match_cols].rename(columns=lambda x: f\"{x}_2\" if x not in ['CIK', 'ryear'] else x),\n",
    "    left_on=['firm2', 'year'],\n",
    "    right_on=['CIK', 'ryear'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop redundant CIK_x columns\n",
    "pair_df.drop(columns=['CIK_x', 'CIK_y', 'ryear_x', 'ryear_y'], inplace=True)\n",
    "\n",
    "# Compute Pair-Level Covariates\n",
    "for col in match_cols:\n",
    "    pair_df[f'{col}_diff'] = abs(pair_df[f'{col}_1'] - pair_df[f'{col}_2'])\n",
    "\n",
    "pair_df.dropna(subset=[f\"{x}_diff\" for x in match_cols], inplace=True)\n",
    "\n",
    "pair_df['SameInd'] = (pair_df['FF_1']==pair_df['FF_2']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propensity Score Estimation + Matching\n",
    "X = pair_df[[f\"{x}_1\" for x in match_cols] + [f\"{x}_2\" for x in match_cols] + ['SameInd']]\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "y = pair_df['treated']\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "pair_df['propensity_score'] = model.fit(X_scaled, y).predict_proba(X_scaled)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match Treated to Control Pairs Using Propensity Score\n",
    "matched_pairs = []\n",
    "\n",
    "pair_df[\"pair_id\"] = pair_df['treated_firm1'].astype(str) + \"_\" + pair_df['treated_firm2'].astype(str)\n",
    "\n",
    "for pair_id in pair_df['pair_id'].unique():\n",
    "    group = pair_df[pair_df['pair_id'] == pair_id]\n",
    "\n",
    "    treated = group[group['treated'] == 1]\n",
    "    controls = group[group['treated'] == 0]\n",
    "\n",
    "    if treated.empty or controls.empty:\n",
    "        continue\n",
    "\n",
    "    treated_score = treated['propensity_score'].values[0]\n",
    "\n",
    "    # Find control pair with closest propensity score\n",
    "    controls.loc[:, 'score_diff'] = (controls['propensity_score'] - treated_score).abs()\n",
    "    best_match = controls.loc[controls['score_diff'].idxmin()]\n",
    "\n",
    "    # Add both treated and matched control to final list\n",
    "    matched_pairs.append(treated.iloc[0].to_dict())\n",
    "    matched_pairs.append(best_match.to_dict())\n",
    "\n",
    "# Convert to Final Matched Sample\n",
    "matched_df = pd.DataFrame(matched_pairs).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove duplicate matched firms and keep matched year\n",
    "trunc_matched_df = matched_df[['firm1', 'firm2', 'treated', 'year']].groupby(['firm1', 'firm2', 'treated'])['year'].max().reset_index()\n",
    "\n",
    "# Ensure matching columns are of the same type (critical)\n",
    "shared_disc_df[['CIK', 'CIK_pair']] = shared_disc_df[['CIK', 'CIK_pair']].astype(int)\n",
    "trunc_matched_df[['firm1', 'firm2']] = trunc_matched_df[['firm1', 'firm2']].astype(int)\n",
    "\n",
    "# Create panel data\n",
    "shared_disc_df = pd.merge(\n",
    "    left=shared_disc_df,\n",
    "    right=trunc_matched_df,\n",
    "    left_on=['CIK', 'CIK_pair'],\n",
    "    right_on=['firm1', 'firm2'],\n",
    "    how='left'\n",
    ").drop(columns=['firm1', 'firm2'])\n",
    "\n",
    "shared_disc_df = pd.merge(\n",
    "    left=shared_disc_df,\n",
    "    right=trunc_matched_df,\n",
    "    left_on=['CIK', 'CIK_pair'],\n",
    "    right_on=['firm2', 'firm1'],\n",
    "    how='left'\n",
    ").drop(columns=['firm1', 'firm2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep only firm-pairs that are either in treatment group or are matched with linked firm-pairs\n",
    "refined_shared_disc_df = shared_disc_df.dropna(subset=[\"treated_x\", \"treated_y\"], how='all')\\\n",
    "    .sort_values(['CIK', 'CIK_pair', 'ryear']).reset_index(drop=True)\n",
    "\n",
    "refined_shared_disc_df.fillna({'year_x': refined_shared_disc_df['year_y']}, inplace=True)\n",
    "refined_shared_disc_df['match_year'] = refined_shared_disc_df['year_x'].astype(int)\n",
    "\n",
    "refined_shared_disc_df.drop(columns=[\"treated_x\", \"treated_y\", \"year_x\", \"year_y\"], inplace=True)\n",
    "\n",
    "# Check if the pair of matched CIKs are actually linked in the sample\n",
    "refined_shared_disc_df[~refined_shared_disc_df['Treatment']]['Linked'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the shared director is ED or Risk Committee in any of the linked firms\n",
    "refined_shared_disc_df = pd.merge(\n",
    "    left=refined_shared_disc_df,\n",
    "    right=link_df.groupby(['CIKCode', 'CIKCode_lnkd', 'Year'])[['ED', 'RiskCommittee', 'AuditCommittee', 'LinkTime']].max(),\n",
    "    left_on=['CIK', 'CIK_pair', 'ryear'],\n",
    "    right_index=True,\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153506, 14)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refined_shared_disc_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linked\n",
       "0    128803\n",
       "1     24703\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refined_shared_disc_df['Linked'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Treatment\n",
       "True     77134\n",
       "False    76372\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refined_shared_disc_df['Treatment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del shared_disc_df\n",
    "del firm_data\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_shared_disc_df.sort_values(['CIK', 'CIK_pair', 'ryear'], inplace=True)\n",
    "\n",
    "# Year when the treated pairs are linked for the first time\n",
    "refined_shared_disc_df['eventX'] = refined_shared_disc_df[refined_shared_disc_df[\"Linked\"]==1]\\\n",
    "    .groupby([\"CIK\", \"CIK_pair\"])['ryear'].transform('min')\n",
    "refined_shared_disc_df['eventX'] = refined_shared_disc_df.groupby(['CIK', 'CIK_pair'])['eventX'].bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep observations in the same range used for DiD \n",
    "refined_shared_disc_df['Time_to_match'] = refined_shared_disc_df['match_year'] - refined_shared_disc_df['ryear']\n",
    "\n",
    "refined_shared_disc_df = refined_shared_disc_df[\n",
    "    (refined_shared_disc_df['Treatment'])\n",
    "    |(refined_shared_disc_df['Time_to_match'].isin(range(-5,10)))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CIK', 'report_dt', 'SharedRF', 'ryear', 'CIK_pair', 'report_dt_pair',\n",
       "       'NoSharedDir', 'Linked', 'Treatment', 'match_year', 'ED',\n",
       "       'RiskCommittee', 'AuditCommittee', 'LinkTime', 'eventX',\n",
       "       'Time_to_match'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refined_shared_disc_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45319, 62)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firm_data = pd.read_csv('Data/Study2_data1_V3.csv', parse_dates=['report_dt', 'filing_dt'])\n",
    "firm_data.dropna(subset=[\n",
    "    'added', 'Volatility_120', 'Beta_126', 'logTA', 'ROE', 'DtA', 'Current', 'RDxopr', 'BtM',\n",
    "    'GenderRatio', 'NUMBEROFANALYSTS', 'Age', 'Independent', 'TotCurrBrd'\n",
    "], inplace=True)\n",
    "\n",
    "firm_data = firm_data[firm_data.groupby('CIK')['ryear'].transform('nunique')>1].reset_index(drop=True)\n",
    "\n",
    "firm_data.drop(\n",
    "    columns=[\n",
    "        'SharedRF', 'ryear', 'LinkTime', 'Degree', 'rmonth', 'cik', 'mkvalt', 'logMC', 'ProprietaryCost', 'IndVol+',\n",
    "        'NumberDirectors_1', 'ShrdDir_1', 'LnkdFirm_1', 'ShrdED_1', 'ShrdRC_1', 'ShrdAC_1', \n",
    "        'Independent_1', 'Volatility_120_1', 'Beta_126_1', 'IndVol__1', 'logTA_1', 'ROE_1', 'DtA_1', 'Current_1',\n",
    "        'RDxopr_1', 'BtM_1', 'rfGap_1', 'Big4_1', 'COUNT_WEAK_1', 'NUMBEROFANALYSTS_1', 'reported_1',\n",
    "        'COUNT_WEAK', 'Big4', 'AUDITOR_FKEY'],\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "firm_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICW = pd.read_csv(\"Data/ICW2.csv\", parse_dates=['FYE_IC_OP', 'FILE_DATE'])\n",
    "\n",
    "ICW.sort_values(['COMPANY_FKEY', 'FILE_DATE'], inplace=True)\n",
    "\n",
    "ICW[\"OPyr\"] = ICW['FYE_IC_OP'].dt.year\n",
    "ICW[\"fyear\"] = ICW['FILE_DATE'].dt.year\n",
    "\n",
    "ICW.dropna(subset=['LAST_AUD_NAME'], inplace=True)\n",
    "\n",
    "# Replace unknown auditors with last known auditors\n",
    "ICW.loc[ICW['OP_AUD_NAME']=='unknown', 'OP_AUD_NAME'] = ICW.loc[ICW['OP_AUD_NAME']=='unknown', 'LAST_AUD_NAME']\n",
    "ICW.loc[ICW['AUDITOR_FKEY']==216, 'AUDITOR_FKEY'] = ICW.loc[ICW['AUDITOR_FKEY']==216, 'LAST_AUD_FKEY']\n",
    "\n",
    "ICW['Big4'] = ICW['OP_AUD_NAME'].str.contains(r'Deloitte|KPMG|Ernst|Pricewaterhouse', case=False).astype(int)\n",
    "\n",
    "ICW_gr = ICW.groupby(['COMPANY_FKEY', 'FILE_DATE'])[['COUNT_WEAK', 'Big4', 'AUDITOR_FKEY']].max().reset_index()\n",
    "ICW_gr2 = ICW.groupby(['COMPANY_FKEY', 'fyear'])[['COUNT_WEAK', 'Big4', 'AUDITOR_FKEY']].max().reset_index()\n",
    "\n",
    "firm_data = pd.merge(\n",
    "    left=firm_data,\n",
    "    right=ICW_gr,\n",
    "    left_on=['CIK', 'filing_dt'],\n",
    "    right_on=['COMPANY_FKEY', 'FILE_DATE'],\n",
    "    how=\"left\"\n",
    ").drop(columns=['COMPANY_FKEY', 'FILE_DATE'])\n",
    "\n",
    "firm_data[['COUNT_WEAK_2', 'Big4_2', 'AUDITOR_FKEY_2']] = pd.merge(\n",
    "    left=firm_data,\n",
    "    right=ICW_gr2,\n",
    "    left_on=['CIK', 'fyear'],\n",
    "    right_on=['COMPANY_FKEY', 'fyear'],\n",
    "    how=\"left\"\n",
    ")[['COUNT_WEAK_y', 'Big4_y', 'AUDITOR_FKEY_y']]\n",
    "\n",
    "firm_data.fillna({'COUNT_WEAK': firm_data['COUNT_WEAK_2']}, inplace=True)\n",
    "firm_data.fillna({'Big4': firm_data['Big4_2']}, inplace=True)\n",
    "firm_data.fillna({'AUDITOR_FKEY': firm_data['AUDITOR_FKEY_2']}, inplace=True)\n",
    "\n",
    "firm_data.drop(columns=['COUNT_WEAK_2', 'Big4_2', 'AUDITOR_FKEY_2'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "Study2_data3 = pd.merge(\n",
    "    left=refined_shared_disc_df,\n",
    "    right=firm_data,\n",
    "    on=[\"CIK\", \"report_dt\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "Study2_data3 = pd.merge(\n",
    "    left=Study2_data3,\n",
    "    right=firm_data,\n",
    "    left_on=[\"CIK_pair\", \"report_dt_pair\"],\n",
    "    right_on=[\"CIK\", \"report_dt\"],\n",
    "    how=\"left\",\n",
    "    suffixes=['', '_2']\n",
    ").drop(columns=['CIK_2', 'report_dt_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Volatility_120', 'Beta_126', 'logTA', 'ROE', 'DtA', 'Current', 'RDxopr', 'BtM',\n",
    "    'GenderRatio', 'NUMBEROFANALYSTS', 'Age', 'Independent', 'TotCurrBrd']\n",
    "\n",
    "Study2_data3.dropna(subset=cols, inplace=True)\n",
    "Study2_data3.dropna(subset=[f\"{c}_2\" for c in cols], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = topics_df[['CIK', 'SIC', 'ryear', 'fyear']].drop_duplicates().reset_index(drop=True)\n",
    "# SIC code of high-tech and regulated industries (Kothari, 2009, Kim 2012)\n",
    "high_tech = []\n",
    "high_tech.extend(range(2833, 2838))\n",
    "high_tech.extend(range(3570, 3577))\n",
    "high_tech.extend(range(3600, 3674))\n",
    "high_tech.extend(range(7370, 7374))\n",
    "high_tech.extend(range(8731, 8734))\n",
    "\n",
    "retail_firms =[] \n",
    "retail_firms.extend(range(5200, 5961))\n",
    "\n",
    "regulated = [4812, 4813, 4833, 4841, 4922, 4923, 4924, 4931, 4941]\n",
    "regulated.extend(range(4811, 4900))\n",
    "\n",
    "df['high_litigation'] = df['SIC'].isin(high_tech + retail_firms + regulated).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Study2_data3 = pd.merge(\n",
    "    left=Study2_data3,\n",
    "    right=df.drop(columns=['SIC', 'ryear']).drop_duplicates(),\n",
    "    on=['CIK', 'fyear'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "Study2_data3['high_litigation_2'] = pd.merge(\n",
    "    left=Study2_data3,\n",
    "    right=df.drop(columns=['SIC', 'ryear']).drop_duplicates(),\n",
    "    left_on=['CIK_pair','fyear_2'],\n",
    "    right_on=['CIK', 'fyear'],\n",
    "    how='left'\n",
    ")['high_litigation_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CIK', 'report_dt', 'SharedRF', 'ryear', 'CIK_pair', 'report_dt_pair',\n",
       "       'NoSharedDir', 'Linked', 'Treatment', 'match_year',\n",
       "       ...\n",
       "       'ROA_2', 'IndVol__2', 'InstOwnership_2', 'GenderRatio_1_2',\n",
       "       'COUNT_WEAK_2', 'Big4_2', 'AUDITOR_FKEY_2', 'CIKpair_ID',\n",
       "       'high_litigation', 'high_litigation_2'],\n",
       "      dtype='object', length=145)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Study2_data3['CIKpair_ID'] = Study2_data3[['CIK', 'CIK_pair']].astype(str).apply(lambda x: '-'.join(x), axis=1)\n",
    "Study2_data3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(139118, 145)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Study2_data3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Study2_data3.to_csv('Data/Study2_data3_V6.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Treatment\n",
       "False    7279\n",
       "True     7302\n",
       "Name: CIKpair_ID, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Study2_data3.groupby(\"Treatment\")['CIKpair_ID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "Study2_data3['SumLinked'] = Study2_data3.groupby('CIKpair_ID')['Linked'].transform('sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Study2_data3 = pd.read_csv('Data/Study2_data3_V6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Treatment\n",
       "True     80073\n",
       "False    79205\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Study2_data3['Treatment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIKpair_ID</th>\n",
       "      <th>Linked</th>\n",
       "      <th>ryear</th>\n",
       "      <th>eventX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33724</th>\n",
       "      <td>1000180-1010552</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "      <td>2013.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33725</th>\n",
       "      <td>1000180-1010552</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>2013.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33726</th>\n",
       "      <td>1000180-1010552</td>\n",
       "      <td>0</td>\n",
       "      <td>2010</td>\n",
       "      <td>2013.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33727</th>\n",
       "      <td>1000180-1010552</td>\n",
       "      <td>0</td>\n",
       "      <td>2011</td>\n",
       "      <td>2013.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33728</th>\n",
       "      <td>1000180-1010552</td>\n",
       "      <td>0</td>\n",
       "      <td>2012</td>\n",
       "      <td>2013.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2501</th>\n",
       "      <td>9984-89089</td>\n",
       "      <td>0</td>\n",
       "      <td>2019</td>\n",
       "      <td>2021.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2502</th>\n",
       "      <td>9984-89089</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "      <td>2021.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>9984-89089</td>\n",
       "      <td>1</td>\n",
       "      <td>2021</td>\n",
       "      <td>2021.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>9984-89089</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>2021.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>9984-89089</td>\n",
       "      <td>0</td>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22240 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            CIKpair_ID  Linked  ryear  eventX\n",
       "33724  1000180-1010552       0   2007  2013.0\n",
       "33725  1000180-1010552       0   2008  2013.0\n",
       "33726  1000180-1010552       0   2010  2013.0\n",
       "33727  1000180-1010552       0   2011  2013.0\n",
       "33728  1000180-1010552       0   2012  2013.0\n",
       "...                ...     ...    ...     ...\n",
       "2501        9984-89089       0   2019  2021.0\n",
       "2502        9984-89089       0   2020  2021.0\n",
       "2503        9984-89089       1   2021  2021.0\n",
       "2504        9984-89089       1   2022  2021.0\n",
       "2505        9984-89089       0   2023     NaN\n",
       "\n",
       "[22240 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Study2_data3[Study2_data3[\"Treatment\"]][['CIKpair_ID', 'Linked', 'ryear', 'eventX']].sort_values(['CIKpair_ID', 'ryear'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Treatment\n",
       "False    104940\n",
       "True      20727\n",
       "Name: CIKpair_ID, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Study2_data3.groupby(\"Treatment\")['CIKpair_ID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Study2_data3 = pd.read_csv('Data/Study2_data3_V3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Study2_data3[Study2_data3['Linked']==1].sort_values(['at', 'at_2', 'SharedRF'], ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIK</th>\n",
       "      <th>report_dt</th>\n",
       "      <th>SharedRF</th>\n",
       "      <th>ryear</th>\n",
       "      <th>CIK_pair</th>\n",
       "      <th>report_dt_pair</th>\n",
       "      <th>NoSharedDir</th>\n",
       "      <th>Linked</th>\n",
       "      <th>Treatment</th>\n",
       "      <th>ED</th>\n",
       "      <th>...</th>\n",
       "      <th>Current_2</th>\n",
       "      <th>TobinQ_2</th>\n",
       "      <th>BtM_2</th>\n",
       "      <th>RDxopr_2</th>\n",
       "      <th>ROA_2</th>\n",
       "      <th>IndVol__2</th>\n",
       "      <th>InstOwnership_2</th>\n",
       "      <th>GenderRatio_1_2</th>\n",
       "      <th>IndDisc_2</th>\n",
       "      <th>CIKpair_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>134411</th>\n",
       "      <td>789019</td>\n",
       "      <td>2013-06-30</td>\n",
       "      <td>7</td>\n",
       "      <td>2013</td>\n",
       "      <td>1403161</td>\n",
       "      <td>2013-09-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.804383</td>\n",
       "      <td>3.391111</td>\n",
       "      <td>0.220371</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.138503</td>\n",
       "      <td>0.790331</td>\n",
       "      <td>49.8214</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.235201</td>\n",
       "      <td>789019-1403161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134412</th>\n",
       "      <td>789019</td>\n",
       "      <td>2014-06-30</td>\n",
       "      <td>3</td>\n",
       "      <td>2014</td>\n",
       "      <td>1403161</td>\n",
       "      <td>2014-09-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.592075</td>\n",
       "      <td>3.161367</td>\n",
       "      <td>0.224824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.140994</td>\n",
       "      <td>0.779506</td>\n",
       "      <td>50.9105</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.241938</td>\n",
       "      <td>789019-1403161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134413</th>\n",
       "      <td>789019</td>\n",
       "      <td>2018-06-30</td>\n",
       "      <td>2</td>\n",
       "      <td>2018</td>\n",
       "      <td>1403161</td>\n",
       "      <td>2018-09-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.611322</td>\n",
       "      <td>4.812926</td>\n",
       "      <td>0.102067</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.148805</td>\n",
       "      <td>1.103598</td>\n",
       "      <td>52.9171</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.274508</td>\n",
       "      <td>789019-1403161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134414</th>\n",
       "      <td>789019</td>\n",
       "      <td>2019-06-30</td>\n",
       "      <td>2</td>\n",
       "      <td>2019</td>\n",
       "      <td>1403161</td>\n",
       "      <td>2019-09-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.563176</td>\n",
       "      <td>5.133312</td>\n",
       "      <td>0.093100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166451</td>\n",
       "      <td>1.073451</td>\n",
       "      <td>54.2717</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.281343</td>\n",
       "      <td>789019-1403161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134415</th>\n",
       "      <td>789019</td>\n",
       "      <td>2020-06-30</td>\n",
       "      <td>3</td>\n",
       "      <td>2020</td>\n",
       "      <td>1403161</td>\n",
       "      <td>2020-09-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.905238</td>\n",
       "      <td>4.603925</td>\n",
       "      <td>0.097196</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.134282</td>\n",
       "      <td>1.469331</td>\n",
       "      <td>55.4155</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.299615</td>\n",
       "      <td>789019-1403161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134416</th>\n",
       "      <td>789019</td>\n",
       "      <td>2022-06-30</td>\n",
       "      <td>3</td>\n",
       "      <td>2022</td>\n",
       "      <td>1403161</td>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.448473</td>\n",
       "      <td>4.319304</td>\n",
       "      <td>0.096346</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.174934</td>\n",
       "      <td>1.959102</td>\n",
       "      <td>58.4640</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.321826</td>\n",
       "      <td>789019-1403161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134417</th>\n",
       "      <td>789019</td>\n",
       "      <td>2023-06-30</td>\n",
       "      <td>4</td>\n",
       "      <td>2023</td>\n",
       "      <td>1403161</td>\n",
       "      <td>2023-09-30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.451727</td>\n",
       "      <td>4.080761</td>\n",
       "      <td>0.104881</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.190864</td>\n",
       "      <td>1.070053</td>\n",
       "      <td>55.5927</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.335330</td>\n",
       "      <td>789019-1403161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 146 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           CIK   report_dt  SharedRF  ryear  CIK_pair report_dt_pair  \\\n",
       "134411  789019  2013-06-30         7   2013   1403161     2013-09-30   \n",
       "134412  789019  2014-06-30         3   2014   1403161     2014-09-30   \n",
       "134413  789019  2018-06-30         2   2018   1403161     2018-09-30   \n",
       "134414  789019  2019-06-30         2   2019   1403161     2019-09-30   \n",
       "134415  789019  2020-06-30         3   2020   1403161     2020-09-30   \n",
       "134416  789019  2022-06-30         3   2022   1403161     2022-09-30   \n",
       "134417  789019  2023-06-30         4   2023   1403161     2023-09-30   \n",
       "\n",
       "        NoSharedDir  Linked  Treatment   ED  ...  Current_2  TobinQ_2  \\\n",
       "134411          NaN       0       True  NaN  ...   1.804383  3.391111   \n",
       "134412          NaN       0       True  NaN  ...   1.592075  3.161367   \n",
       "134413          NaN       0       True  NaN  ...   1.611322  4.812926   \n",
       "134414          NaN       0       True  NaN  ...   1.563176  5.133312   \n",
       "134415          NaN       0       True  NaN  ...   1.905238  4.603925   \n",
       "134416          1.0       1       True  0.0  ...   1.448473  4.319304   \n",
       "134417          1.0       1       True  0.0  ...   1.451727  4.080761   \n",
       "\n",
       "           BtM_2  RDxopr_2     ROA_2 IndVol__2  InstOwnership_2  \\\n",
       "134411  0.220371       0.0  0.138503  0.790331          49.8214   \n",
       "134412  0.224824       0.0  0.140994  0.779506          50.9105   \n",
       "134413  0.102067       0.0  0.148805  1.103598          52.9171   \n",
       "134414  0.093100       0.0  0.166451  1.073451          54.2717   \n",
       "134415  0.097196       0.0  0.134282  1.469331          55.4155   \n",
       "134416  0.096346       0.0  0.174934  1.959102          58.4640   \n",
       "134417  0.104881       0.0  0.190864  1.070053          55.5927   \n",
       "\n",
       "        GenderRatio_1_2  IndDisc_2      CIKpair_ID  \n",
       "134411            0.700   0.235201  789019-1403161  \n",
       "134412            0.700   0.241938  789019-1403161  \n",
       "134413            0.800   0.274508  789019-1403161  \n",
       "134414            0.700   0.281343  789019-1403161  \n",
       "134415            0.700   0.299615  789019-1403161  \n",
       "134416            0.667   0.321826  789019-1403161  \n",
       "134417            0.636   0.335330  789019-1403161  \n",
       "\n",
       "[7 rows x 146 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Study2_data3[(Study2_data3['CIK']==789019)&(Study2_data3['CIK_pair']==1403161)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttest_func(x):\n",
    "    A = x['IndDisc'].values\n",
    "    B = x['OtherIndDisc'].values\n",
    "\n",
    "    tstat = stats.ttest_ind(a=A, b=B, equal_var=False).statistic\n",
    "    pvalue = stats.ttest_ind(a=A, b=B, equal_var=False).pvalue\n",
    "\n",
    "    return (tstat, pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_cols = [\n",
    "    'Volatility_120', 'Beta_126', 'logTA', 'ROE', 'DtA', 'Current', 'RDxopr', 'BtM', 'rfGap', 'COUNT_WEAK', 'Big4',\n",
    "    'GenderRatio', 'NUMBEROFANALYSTS', 'NumberDirectors', 'IndVol_', 'Age', 'Independent'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Study2_data3[['CIK', 'ryear', \"CIK_pair\", 'reported', 'reported_2', 'SharedRF', 'Linked', 'Treatment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in match_cols:\n",
    "    df[f\"pair{col}\"] = (Study2_data3[col] + Study2_data3[f\"{col}_2\"])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_cols = [f\"pair{col}\" for col in match_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "des_stats = df.groupby('Treatment').describe()[pair_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Treatment</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">pairVolatility_120</th>\n",
       "      <th>mean</th>\n",
       "      <td>0.235911</td>\n",
       "      <td>56.442966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.504195</td>\n",
       "      <td>12136.653986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">pairBeta_126</th>\n",
       "      <th>mean</th>\n",
       "      <td>1.460063</td>\n",
       "      <td>88.239086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>66.848763</td>\n",
       "      <td>128242.300487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">pairlogTA</th>\n",
       "      <th>mean</th>\n",
       "      <td>6.315588</td>\n",
       "      <td>7.480883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.580427</td>\n",
       "      <td>1.863061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">pairROE</th>\n",
       "      <th>mean</th>\n",
       "      <td>0.372844</td>\n",
       "      <td>0.324211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>34.666789</td>\n",
       "      <td>36.645229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">pairDtA</th>\n",
       "      <th>mean</th>\n",
       "      <td>0.271379</td>\n",
       "      <td>0.280118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.775053</td>\n",
       "      <td>0.299748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">pairCurrent</th>\n",
       "      <th>mean</th>\n",
       "      <td>3.374045</td>\n",
       "      <td>3.545643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.033627</td>\n",
       "      <td>73.955514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">pairRDxopr</th>\n",
       "      <th>mean</th>\n",
       "      <td>0.128029</td>\n",
       "      <td>0.137004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.222842</td>\n",
       "      <td>0.294373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">pairBtM</th>\n",
       "      <th>mean</th>\n",
       "      <td>1.033265</td>\n",
       "      <td>0.340119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>46.437754</td>\n",
       "      <td>30.265715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">pairrfGap</th>\n",
       "      <th>mean</th>\n",
       "      <td>65.120415</td>\n",
       "      <td>58.718594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>16.951265</td>\n",
       "      <td>13.824917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">pairCOUNT_WEAK</th>\n",
       "      <th>mean</th>\n",
       "      <td>0.186150</td>\n",
       "      <td>0.103171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.639287</td>\n",
       "      <td>0.496647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">pairBig4</th>\n",
       "      <th>mean</th>\n",
       "      <td>0.625545</td>\n",
       "      <td>0.813589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.344399</td>\n",
       "      <td>0.313824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">pairGenderRatio</th>\n",
       "      <th>mean</th>\n",
       "      <td>0.855489</td>\n",
       "      <td>0.821582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.103046</td>\n",
       "      <td>0.107258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">pairNUMBEROFANALYSTS</th>\n",
       "      <th>mean</th>\n",
       "      <td>7.848719</td>\n",
       "      <td>11.551291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.741965</td>\n",
       "      <td>7.642017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">pairNumberDirectors</th>\n",
       "      <th>mean</th>\n",
       "      <td>8.027845</td>\n",
       "      <td>9.169944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.535938</td>\n",
       "      <td>1.780059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">pairIndVol_</th>\n",
       "      <th>mean</th>\n",
       "      <td>1.379780</td>\n",
       "      <td>1.359479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.599186</td>\n",
       "      <td>0.614996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">pairAge</th>\n",
       "      <th>mean</th>\n",
       "      <td>60.937327</td>\n",
       "      <td>61.154136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.468624</td>\n",
       "      <td>3.214408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">pairIndependent</th>\n",
       "      <th>mean</th>\n",
       "      <td>6.309940</td>\n",
       "      <td>7.567589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.649669</td>\n",
       "      <td>1.916218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Treatment                      False          True \n",
       "pairVolatility_120   mean   0.235911      56.442966\n",
       "                     std    7.504195   12136.653986\n",
       "pairBeta_126         mean   1.460063      88.239086\n",
       "                     std   66.848763  128242.300487\n",
       "pairlogTA            mean   6.315588       7.480883\n",
       "                     std    1.580427       1.863061\n",
       "pairROE              mean   0.372844       0.324211\n",
       "                     std   34.666789      36.645229\n",
       "pairDtA              mean   0.271379       0.280118\n",
       "                     std    0.775053       0.299748\n",
       "pairCurrent          mean   3.374045       3.545643\n",
       "                     std    4.033627      73.955514\n",
       "pairRDxopr           mean   0.128029       0.137004\n",
       "                     std    0.222842       0.294373\n",
       "pairBtM              mean   1.033265       0.340119\n",
       "                     std   46.437754      30.265715\n",
       "pairrfGap            mean  65.120415      58.718594\n",
       "                     std   16.951265      13.824917\n",
       "pairCOUNT_WEAK       mean   0.186150       0.103171\n",
       "                     std    0.639287       0.496647\n",
       "pairBig4             mean   0.625545       0.813589\n",
       "                     std    0.344399       0.313824\n",
       "pairGenderRatio      mean   0.855489       0.821582\n",
       "                     std    0.103046       0.107258\n",
       "pairNUMBEROFANALYSTS mean   7.848719      11.551291\n",
       "                     std    5.741965       7.642017\n",
       "pairNumberDirectors  mean   8.027845       9.169944\n",
       "                     std    1.535938       1.780059\n",
       "pairIndVol_          mean   1.379780       1.359479\n",
       "                     std    0.599186       0.614996\n",
       "pairAge              mean  60.937327      61.154136\n",
       "                     std    3.468624       3.214408\n",
       "pairIndependent      mean   6.309940       7.567589\n",
       "                     std    1.649669       1.916218"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = des_stats.loc(axis=1)[:, ['mean', 'std']].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PhD_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
